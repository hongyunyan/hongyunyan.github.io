<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration CVPR 2019 Oral [2019.7.14]  本文主要的想法是说传统的filter pruning的方式，都是去掉范数值较小的filter，但是实际上通过删除范数较小的filter来进行压缩的方式，需要满足两个条件">
<meta property="og:type" content="article">
<meta property="og:title" content="Paper Reading Go On">
<meta property="og:url" content="http://yoursite.com/2020/01/29/filter-pruning/index.html">
<meta property="og:site_name" content="Yunyan Hong">
<meta property="og:description" content="Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration CVPR 2019 Oral [2019.7.14]  本文主要的想法是说传统的filter pruning的方式，都是去掉范数值较小的filter，但是实际上通过删除范数较小的filter来进行压缩的方式，需要满足两个条件">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/1.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/2.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/3.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/4.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/5.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/6.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/7.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/8.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/9.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/10.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/11.png">
<meta property="og:image" content="http://yoursite.com/2020/01/29/filter-pruning/12.png">
<meta property="og:updated_time" content="2020-02-29T07:20:09.418Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Paper Reading Go On">
<meta name="twitter:description" content="Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration CVPR 2019 Oral [2019.7.14]  本文主要的想法是说传统的filter pruning的方式，都是去掉范数值较小的filter，但是实际上通过删除范数较小的filter来进行压缩的方式，需要满足两个条件">
<meta name="twitter:image" content="http://yoursite.com/2020/01/29/filter-pruning/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/01/29/filter-pruning/">





  <title>Paper Reading Go On | Yunyan Hong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yunyan Hong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/01/29/filter-pruning/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yunyan.hong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yunyan Hong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Paper Reading Go On</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-01-29T16:52:19+08:00">
                2020-01-29
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h4 id="Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration"><a href="#Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration" class="headerlink" title="Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration"></a>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration</h4><ul>
<li>CVPR 2019 Oral [2019.7.14]</li>
</ul>
<p>本文主要的想法是说传统的filter pruning的方式，都是去掉范数值较小的filter，但是实际上通过删除范数较小的filter来进行压缩的方式，需要满足两个条件：</p>
<ul>
<li>网络中filter的范数值分布要较大</li>
<li>范数值较小的那些filter的范数值应该非常小</li>
</ul>
<p>但是实际上绝大多数的网络并不具有这个条件，因此我们通过删除范数小的剪枝方法也存在问题 。因此作者提出FPGM的方法，来删除每层中可替代的 filter（也就是那些包含了冗余信息的filter）。</p>
<p>对于找到最可替代的filter，这边采用的方式为几何中位数的衡量方式，找出离其他filter距离都比较小的filter（也就是比较可以被其他filter一起表示的信息，差不多就是比较中心的位置）来进行压缩裁剪。</p>
<p>对于实验部分，作者比较了Cifar10和ImageNet两个数据集下的表现，并且不仅实验了自己的FPGM方法，也实验了FPGM和norm-based混合的方法。然后选用了以前的MIL, PFEC, CP, ThiNet, SFP, NISP, Re-thinking这几种filter pruning方法进行对比，选用的模型是对于Cifar10，用不同层数的VGGNet和ResNet，对于ImageNet选用不同层数的ResNet。下图分别是ResNet在cifar10上的表现和在ImageNet上的表现结果。</p>
<p><img src="/2020/01/29/filter-pruning/1.png" width="80%"></p>
<p><img src="/2020/01/29/filter-pruning/2.png" width="80%"></p>
<p>​    另外，作者 把 ResNet-50的第一个卷积层的特征图进行了可视化。可以发现被去掉的通道特征主要有两部分。1）竹子的骨架，2）熊猫的轮廓。其中竹子和熊猫的骨架都可以被剩下的通道代替，验证了我们基于几何中心进行滤波器剪枝的理论。</p>
<p><br></p>
<p><strong>文章的代码情况</strong>：</p>
<ul>
<li>试了从头prune在cifar10上的测试，可以跑通</li>
<li>有origin_model在imagenet上的代码，不能全部跑通，初步认为是数据载入有问题，用imagenet部分数据实验，后续进行没有问题，另外没有在cifar10上的代码，后续自己补全</li>
<li>没有试过在pretrain的model上prune的代码，但应该可以跑通，整体代码内容比较全。</li>
<li>代码地址：<a href="https://github.com/he-y/filter-pruning-geometric-median" target="_blank" rel="noopener">https://github.com/he-y/filter-pruning-geometric-median</a></li>
</ul>
<p><br></p>
<h4 id="Soft-Filter-Pruning-for-Accelerating-Deep-Convolutional-Neural-Networks"><a href="#Soft-Filter-Pruning-for-Accelerating-Deep-Convolutional-Neural-Networks" class="headerlink" title="Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks"></a>Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks</h4><ul>
<li>IJCAI 2018</li>
<li>和上一篇文章同一个一作</li>
<li>就是SFP方法，感觉应该也是目前filter pruning的开篇之一了</li>
</ul>
<p>网上解读</p>
<blockquote>
<p> <a href="https://blog.csdn.net/u014380165/article/details/81107032" target="_blank" rel="noopener">https://blog.csdn.net/u014380165/article/details/81107032</a></p>
<p> <a href="https://blog.csdn.net/h__ang/article/details/89600279" target="_blank" rel="noopener">https://blog.csdn.net/h__ang/article/details/89600279</a></p>
</blockquote>
<p>基本的思路如下图所示：就是每一轮找出范式值最小的k个，然后置为0，带入下一轮训练，所以每轮置为0的会有变化。而不是HFP中一次性暴力删除最小的那些filter。【BTW，感觉这个作者论文写的很清楚，实验也给的很详细，代码也能work，表格也很全】</p>
<p><img src="/2020/01/29/filter-pruning/3.png" width="100%"></p>
<p>SFP有两大优势：</p>
<ul>
<li>更大的模型容量：对之前剪掉的卷积核更新使得我们的方法相比于固定卷积核为0，有更大的网络容量。因此，通过我们的方法训练得到的网络由于有更大的网络容量可以从训练数据学得更好。</li>
<li>对预训练模型的依赖减弱：大的网络容量使得SFP可以从头开始训练，同时对模型进行剪枝。相反，之前的卷积核剪枝方法是在预训练模型的基础指导之下保证网络性能。而从头开始训练的SFP优于之前的卷积核剪枝方法。</li>
</ul>
<p>代码就先不测了，看了一下，和上面那个代码同一拨操作的，后续用到了再检测吧。</p>
<p>地址：<a href="https://github.com/he-y/soft-filter-pruning" target="_blank" rel="noopener">https://github.com/he-y/soft-filter-pruning</a></p>
<p><br></p>
<blockquote>
<p> 感觉下一阶段的文章画风有发生了些许变化，因为要去找个合理的应用 ，并且了解这个case的现况，所以要读一堆视频类的文章</p>
</blockquote>
<p><br></p>
<h4 id="AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition"><a href="#AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition" class="headerlink" title="AdaFrame: Adaptive Frame Selection for Fast Video Recognition"></a>AdaFrame: Adaptive Frame Selection for Fast Video Recognition</h4><ul>
<li>CVPR 2019</li>
</ul>
<p>这篇文章是讲我来尽量少的选择视频中的帧，来做一个快速的video recognition。</p>
<p><img src="/2020/01/29/filter-pruning/4.png" width="100%"></p>
<p>主要的流程就是这张图。首先右边的部分是作者用了一个轻量级的CNN对video中下采样的frame做一个处理，活得一个global context。然后对于左边的每张图，在当前frame上 ，先用一个CNN，来提取这个frame的信息，然后结合global context和上一帧（上一个被选中的帧）的信息扔到LSTM里面去计算，获得hidden state，然后根据这个hidden state分别用于prediction network，selection network和utility network。</p>
<p>prediction network部分就是先算出对各个class 的probility，然后进一步计算reward，reward可以理解为ground truth的probility 减去其他类中probility最大的那个，用这个reward来激励看到更多的frame后获得更大的confidence。</p>
<p>selection network是用来选择下一个frame是哪一帧。用一个高斯分布的想法，加上计算得到的中心位置，来选择下一帧，训练网络的函数是最大化未来的reward。</p>
<p>utility network，这是根据现有的情况来估计未来的reward，来看要不要继续看下面的frame。</p>
<p>对于在inference中，选择什么时候停止的方式是根据utility的情况来决定的，设定一个超参数miu，当计算得到的utility 值和最大的utility值的差值大于miu达到一定次数，就结束。</p>
<p>对于实验部分，文章和avgpooling的方法以及LSTM的方法，以及另外两个选frame的fast forward以及frameGlimpse方法进行比较，证明了自己可以看少量frame而达到一个很好的判断。并且对于简单的case，需要看的帧越少。</p>
<p><img src="/2020/01/29/filter-pruning/5.png" width="100%"></p>
<p>整体感觉，在这篇文章前，做这个选帧的应该不多，但不知道一年过去了有什么进展。因此这边的baseline也非常的普通吧。而且数据集选择的也是这两个数据集（不知道为什么不是那些常见的，是不是因为那些视频 太过于短了。）</p>
<p><br></p>
<h4 id="Multi-Agent-Reinforcement-Learning-Based-Frame-Sampling-for-Effective-Untrimmed-Video-Recognition"><a href="#Multi-Agent-Reinforcement-Learning-Based-Frame-Sampling-for-Effective-Untrimmed-Video-Recognition" class="headerlink" title="Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition"></a>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</h4><ul>
<li>ICCV 2019</li>
</ul>
<p>文章的主要目标也是选frame来做video recognition。</p>
<p><img src="/2020/01/29/filter-pruning/6.png" width="100%"></p>
<p>文章这边整体模型的思路是，我有M个agent，然后每个agent初始状态下均匀分布在所有帧中，然后第一帧、最后一帧以及每个agent选的帧通过网络转化成feature vector，然后每个agent，将自己的信息+前后M个被选帧的信息结合起来，经过GRU得到hidden state，然后再进入policy network做下一个action的预测，当所有的agent的选择都是stop的时候，整个过程结束。</p>
<p>然后再训练的时候，另外加入了loss function来帮助agent不要很快stop，encourages exploration。</p>
<p>对于实验部分，他和楼上文章以及楼上文章的两个baseline进行了对比，并且组合了多种backbones，进行实验。同时也和uniform以及random选帧以及ALL frame的方式进行比较效果。证明自己的优越性。</p>
<p>另外，实验发现agent的数目即使逼近frame的个数，效果也比ALL frame好，因为他可以将很多无效信息删除，来提高效果。</p>
<p><br></p>
<h4 id="BubbleNets-Learning-to-Select-the-Guidance-Frame-in-Video-Object-Segmentation-by-Deep-Sorting-Frames"><a href="#BubbleNets-Learning-to-Select-the-Guidance-Frame-in-Video-Object-Segmentation-by-Deep-Sorting-Frames" class="headerlink" title="BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames"></a>BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames</h4><ul>
<li>CVPR 2019</li>
</ul>
<p>这篇文章也是选frame，但不同的是他只需要选一个frame，同时他的应用case不是video recognition，而是video object segmentation。</p>
<p>他整体的思路就是一个冒泡排序，从前到后开始比较相邻两帧，然后找出其中最好的那帧。从下图中可以看出，除了被选择比较的两帧，还有几帧reference frames用来帮助做判断。大致的思路就是这样，没有具体细看。</p>
<p><img src="/2020/01/29/filter-pruning/7.png" width="60%"></p>
<p><br></p>
<h4 id="End-to-End-Learning-of-Action-Detection-from-Frame-Glimpses-in-Videos"><a href="#End-to-End-Learning-of-Action-Detection-from-Frame-Glimpses-in-Videos" class="headerlink" title="End-to-End Learning of Action Detection from Frame Glimpses in Videos"></a>End-to-End Learning of Action Detection from Frame Glimpses in Videos</h4><ul>
<li>CVPR 2016 </li>
<li>LI Fei Fei组工作</li>
</ul>
<p>主要是来detection video中哪些部分是有action的。看完这篇文章可以感觉AdaFrame是有点参考了他的想法的，都是一次次帧的选择，有reward，有prediction。</p>
<p><a href="https://www.cnblogs.com/wangxiaocvpr/p/6837079.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangxiaocvpr/p/6837079.html</a></p>
<p><br></p>
<h4 id="Watching-a-Small-Portion-could-be-as-Good-as-Watching-All-Towards-Efficient-Video-Classification"><a href="#Watching-a-Small-Portion-could-be-as-Good-as-Watching-All-Towards-Efficient-Video-Classification" class="headerlink" title="Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification"></a>Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification</h4><ul>
<li>IJCAI 2018</li>
</ul>
<p>也是一篇选帧的文章，这应该就是前面的对比实验 fast forward。整体思路区别不大，这边每次可以选择fast forward来判断前进多少帧或后退多少帧，或者选用一个网络来判断是否停止（这个就不知道具体前进几帧怎么搞）。选择的方式还是大同小异的，具体的损失和reward函数也没细看。总之看到现在，所有的方法都换汤不换药。</p>
<p><img src="/2020/01/29/filter-pruning/8.png" width="100%"></p>
<p><br></p>
<p>下面是几篇很经典的video recognition的文章。</p>
<p><br></p>
<h4 id="Two-stream-convolutional-networks-for-action-recognition-in-videos"><a href="#Two-stream-convolutional-networks-for-action-recognition-in-videos" class="headerlink" title="Two-stream convolutional networks for action recognition in videos"></a>Two-stream convolutional networks for action recognition in videos</h4><ul>
<li>NIPS 2014</li>
</ul>
<p>主要就是用两个支路，一个获取空间信息，一个获取时间信息。</p>
<p><img src="/2020/01/29/filter-pruning/9.png" width="80%"></p>
<p>论文详解：<a href="https://zhuanlan.zhihu.com/p/31841417" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31841417</a></p>
<p><a href="https://blog.csdn.net/zhangjunhit/article/details/77991038" target="_blank" rel="noopener">https://blog.csdn.net/zhangjunhit/article/details/77991038</a></p>
<p><br></p>
<h4 id="Temporal-segment-networks-Towards-good-practices-for-deep-action-recognition"><a href="#Temporal-segment-networks-Towards-good-practices-for-deep-action-recognition" class="headerlink" title="Temporal segment networks: Towards good practices for deep action recognition"></a>Temporal segment networks: Towards good practices for deep action recognition</h4><ul>
<li>ECCV 2016 </li>
<li>就是TSN算法</li>
</ul>
<blockquote>
<p>这篇ECCV2016的文章主要提出TSN（temporal segment network）结构用来做视频的动作识别。TSN可以看做是双流（two stream）系列的改进,  在此基础上，文章要解决两个问题：1、是长时间视频的行为判断问题（有些视频的动作时间较长）。2、是解决数据少的问题，数据量少会使得一些深层的网络难以应用到视频数据中，因为过拟合会比较严重。</p>
<p>针对第一个问题，首先，为什么目前的双流结构网络难以学习到视频的长时间信息？因为其针对的主要是单帧图像或者短时间内的一堆帧图像数据，但这对于时间跨度较长的视频动作检测而言是不够的。因此采用更加密集的图像帧采样方式来获取视频的长时间信息是比较常用的方法，但是这样做会增加不少时间成本，同时作者发现视频的连续帧之间存在冗余，因此想到用稀疏采样代替密集采样，也就是说在对视频做抽帧的时候采取较为稀疏的抽帧方式，这样可以去除一些冗余信息，同时降低了计算量。</p>
<p>针对第二个问题，可通过常规的数据增强方式，比如随机裁剪，水平翻转等；另外还有作者提到的交叉预训练，dropout等方式来减少过拟合。这些后面会详细解释。</p>
</blockquote>
<p>我感觉粗暴的理解就是多截了几段，稀疏抽针，去冗余信息。</p>
<p><img src="/2020/01/29/filter-pruning/10.png" width="100%"></p>
<p>论文详解：<a href="https://blog.csdn.net/u014380165/article/details/79029309" target="_blank" rel="noopener">https://blog.csdn.net/u014380165/article/details/79029309</a></p>
<p><br></p>
<h4 id="Non-local-neural-networks"><a href="#Non-local-neural-networks" class="headerlink" title="Non-local neural networks"></a>Non-local neural networks</h4><ul>
<li>Kaiming He 2017</li>
</ul>
<p>论文解读：<a href="https://blog.csdn.net/elaine_bao/article/details/80821306" target="_blank" rel="noopener">https://blog.csdn.net/elaine_bao/article/details/80821306</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33345791" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33345791</a></p>
<p><br></p>
<h4 id="Compressed-Video-Action-Recognition"><a href="#Compressed-Video-Action-Recognition" class="headerlink" title="Compressed Video Action Recognition"></a>Compressed Video Action Recognition</h4><ul>
<li>CVPR 2018</li>
</ul>
<p>这篇文章将视频压缩以后再进行action recognition，根据压缩视频得到的I帧和P帧，进行识别。</p>
<p>论文解读：<a href="https://blog.csdn.net/b224618/article/details/82711707" target="_blank" rel="noopener">https://blog.csdn.net/b224618/article/details/82711707</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/38626319" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38626319</a></p>
<p><br></p>
<h4 id="Real-time-Action-Recognition-with-Enhanced-Motion-Vector-CNNs"><a href="#Real-time-Action-Recognition-with-Enhanced-Motion-Vector-CNNs" class="headerlink" title="Real-time Action Recognition with Enhanced Motion Vector CNNs"></a>Real-time Action Recognition with Enhanced Motion Vector CNNs</h4><ul>
<li>CVPR 2016</li>
</ul>
<p>用motion vector来代替optimal flow，改善two-stream，但由于 motion vector信息缺失，需要optimal flow作为teacher net来进行指导。</p>
<p>论文解读：<a href="https://blog.csdn.net/AUTO1993/article/details/78315175" target="_blank" rel="noopener">https://blog.csdn.net/AUTO1993/article/details/78315175</a></p>
<p><br></p>
<h4 id="ECO-Efficient-Convolutional-Network-for-Online-Video-Understanding"><a href="#ECO-Efficient-Convolutional-Network-for-Online-Video-Understanding" class="headerlink" title="ECO: Efficient Convolutional Network for Online Video Understanding"></a>ECO: Efficient Convolutional Network for Online Video Understanding</h4><ul>
<li>ECCV 2018</li>
</ul>
<p>解读写的是真的清楚明了啊orz <a href="https://zhuanlan.zhihu.com/p/36795554" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36795554</a></p>
<p>简单而言就是，先将视频分成几个clip，然后随机抽一帧，过2D的卷积，然后将每段clip的卷积结果拼接，过3D卷积（以及2D卷积，组合）获得结果。和TSN有所类似，就是融合方式不同。</p>
<p><img src="/2020/01/29/filter-pruning/11.png" width="100%"></p>
<p><img src="/2020/01/29/filter-pruning/12.png" width="100%"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/02/6-828/" rel="next" title="6.828课程学习">
                <i class="fa fa-chevron-left"></i> 6.828课程学习
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/25/杂货铺/" rel="prev" title="CS相关七七八八杂货铺">
                CS相关七七八八杂货铺 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
     <div id="gitalk-container"></div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yunyan.hong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Filter-Pruning-via-Geometric-Median-for-Deep-Convolutional-Neural-Networks-Acceleration"><span class="nav-number">1.</span> <span class="nav-text">Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Soft-Filter-Pruning-for-Accelerating-Deep-Convolutional-Neural-Networks"><span class="nav-number">2.</span> <span class="nav-text">Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#AdaFrame-Adaptive-Frame-Selection-for-Fast-Video-Recognition"><span class="nav-number">3.</span> <span class="nav-text">AdaFrame: Adaptive Frame Selection for Fast Video Recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-Agent-Reinforcement-Learning-Based-Frame-Sampling-for-Effective-Untrimmed-Video-Recognition"><span class="nav-number">4.</span> <span class="nav-text">Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BubbleNets-Learning-to-Select-the-Guidance-Frame-in-Video-Object-Segmentation-by-Deep-Sorting-Frames"><span class="nav-number">5.</span> <span class="nav-text">BubbleNets: Learning to Select the Guidance Frame in Video Object Segmentation by Deep Sorting Frames</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#End-to-End-Learning-of-Action-Detection-from-Frame-Glimpses-in-Videos"><span class="nav-number">6.</span> <span class="nav-text">End-to-End Learning of Action Detection from Frame Glimpses in Videos</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Watching-a-Small-Portion-could-be-as-Good-as-Watching-All-Towards-Efficient-Video-Classification"><span class="nav-number">7.</span> <span class="nav-text">Watching a Small Portion could be as Good as Watching All: Towards Efficient Video Classification</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Two-stream-convolutional-networks-for-action-recognition-in-videos"><span class="nav-number">8.</span> <span class="nav-text">Two-stream convolutional networks for action recognition in videos</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Temporal-segment-networks-Towards-good-practices-for-deep-action-recognition"><span class="nav-number">9.</span> <span class="nav-text">Temporal segment networks: Towards good practices for deep action recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Non-local-neural-networks"><span class="nav-number">10.</span> <span class="nav-text">Non-local neural networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Compressed-Video-Action-Recognition"><span class="nav-number">11.</span> <span class="nav-text">Compressed Video Action Recognition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Real-time-Action-Recognition-with-Enhanced-Motion-Vector-CNNs"><span class="nav-number">12.</span> <span class="nav-text">Real-time Action Recognition with Enhanced Motion Vector CNNs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ECO-Efficient-Convolutional-Network-for-Online-Video-Understanding"><span class="nav-number">13.</span> <span class="nav-text">ECO: Efficient Convolutional Network for Online Video Understanding</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yunyan.hong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '214b2aee446518ccee23',
          clientSecret: '6b5e9007abf73fbcdce7d46cc4ae66a5fa5e6a3d',
          repo: 'hongyunyan.github.io',
          owner: 'hongyunyan',
          admin: ['hongyunyan'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
