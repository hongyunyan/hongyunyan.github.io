<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="A Neural Attention Model for Abstractive Sentence Summariztion 文章地址：https://www.aclweb.org/anthology/D/D15/D15-1044.pdf   Alexander M. Rush et al., Facebook AI Research/Harvard EMNLP2015  文章利用Seq2Seq模">
<meta property="og:type" content="article">
<meta property="og:title" content="Abstractive Summarization 扫文清单">
<meta property="og:url" content="http://yoursite.com/2019/02/13/扫文笔记2/index.html">
<meta property="og:site_name" content="Yunyan Hong">
<meta property="og:description" content="A Neural Attention Model for Abstractive Sentence Summariztion 文章地址：https://www.aclweb.org/anthology/D/D15/D15-1044.pdf   Alexander M. Rush et al., Facebook AI Research/Harvard EMNLP2015  文章利用Seq2Seq模">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/1.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/2.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/3.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/4.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/5.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/6.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/7.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/8.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/9.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/10.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/11.png">
<meta property="og:image" content="http://yoursite.com/2019/02/13/扫文笔记2/12.png">
<meta property="og:updated_time" content="2019-02-15T08:19:15.584Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Abstractive Summarization 扫文清单">
<meta name="twitter:description" content="A Neural Attention Model for Abstractive Sentence Summariztion 文章地址：https://www.aclweb.org/anthology/D/D15/D15-1044.pdf   Alexander M. Rush et al., Facebook AI Research/Harvard EMNLP2015  文章利用Seq2Seq模">
<meta name="twitter:image" content="http://yoursite.com/2019/02/13/扫文笔记2/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/02/13/扫文笔记2/">





  <title>Abstractive Summarization 扫文清单 | Yunyan Hong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yunyan Hong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/13/扫文笔记2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yunyan.hong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yunyan Hong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Abstractive Summarization 扫文清单</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-13T13:53:26+08:00">
                2019-02-13
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h4 id="A-Neural-Attention-Model-for-Abstractive-Sentence-Summariztion"><a href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summariztion" class="headerlink" title="A Neural Attention Model for Abstractive Sentence Summariztion"></a>A Neural Attention Model for Abstractive Sentence Summariztion</h4><ul>
<li>文章地址：<a href="https://www.aclweb.org/anthology/D/D15/D15-1044.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D/D15/D15-1044.pdf</a></li>
</ul>
<ul>
<li>Alexander M. Rush et al., Facebook AI Research/Harvard</li>
<li>EMNLP2015</li>
</ul>
<p>文章利用Seq2Seq模型来做abstractive summarization。提出三种encoder模式和decoder模式。文章整个模型框架以及主体公式如下图</p>
<p><img src="/2019/02/13/扫文笔记2/1.png" width="50%" height="50%"></p>
<p><img src="/2019/02/13/扫文笔记2/2.png" width="50%" height="50%"></p>
<p>对于左图的encoder部分，作者提出了三种encoder的方法，分别为</p>
<ol>
<li><p>Bag-of-Words Encoder</p>
<p>这个encoder赋予每个词相同的权重，忽略了词序问题。整个encoder中只有F (word Emedding Matrix) 是parameter</p>
<p><img src="/2019/02/13/扫文笔记2/3.png" width="50%" height="50%"></p>
</li>
<li><p>Convolutional Encoder</p>
<p>F是Word Embedding Matrix，Q是卷积核，可以理解为卷积模型。整体考虑的是邻近单词的互动</p>
<p><img src="/2019/02/13/扫文笔记2/4.png" width="50%" height="50%"></p>
</li>
<li><p>Attention-Based Encoder</p>
<p>整体和BoW类似，只是把Uniform分布的部分换成了input和summary之间的soft alignment值，即上下文能对齐的part提供比较高的权重（来自机器翻译的思想）</p>
<p><img src="/2019/02/13/扫文笔记2/5.png" width="50%" height="50%"></p>
</li>
</ol>
<p>对于Generation部分，文章采用Beam Search的方法。即给定一个beam size K，在生成每一个summary word时，都保留概率最大的K个词。直到生成的词是EOS或者达到最大句子长度限制。最后得到的结果是K个最好的sentence summary。</p>
<p><img src="/2019/02/13/扫文笔记2/6.png" width="50%" height="50%"></p>
<p>性能结果（整个架构模型不加上trick部分）</p>
<ul>
<li>DUC-2004: <strong>Rouge-1</strong> 26.55  <strong>Rouge-2</strong> 7.06  <strong>Rouge-L</strong> 22.05</li>
<li>Gigaword:  <strong>Rouge-1</strong> 30.88 <strong>Rouge-2</strong> 12.65 <strong>Rouge-L</strong> 28.34</li>
</ul>
<p><br></p>
<h4 id="Abstractive-Sentence-Summarization-with-Attentive-Recurrent-Neural-Networks"><a href="#Abstractive-Sentence-Summarization-with-Attentive-Recurrent-Neural-Networks" class="headerlink" title="Abstractive Sentence Summarization with Attentive Recurrent Neural Networks"></a>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</h4><ul>
<li>文章地址：<a href="http://www.aclweb.org/anthology/N16-1012" target="_blank" rel="noopener">http://www.aclweb.org/anthology/N16-1012</a></li>
</ul>
<ul>
<li>Sumit Chopra et al., Facebook AI Research【和上一篇是姊妹篇】</li>
<li>NAACL2016</li>
</ul>
<p>文章模型的感觉是利用一个attentive的encoder进行encoder，然后正常的rnn进行decoder。用现在的attention model模型来看的意思，这边的$k!=v$。即创造attention vector的input部分并不是encoder中真正的encoder（就是不是纯的每个位置的word embedding vector），而是</p>
<ol>
<li><p>先将每个位置xi的word embedding vector和表示位置信息的vector进行相加，作为每个位置词对应的vector，记为$a_i ( 1\times d) $</p>
</li>
<li><p>然后利用一个一维卷积核做一个卷积操作，就k个$ B_{d \times q} $矩阵, $z_{ik} = \sum a_i \cdot B^k$, $z_i = [z_1,z_2,…,z_k]$, $z_i$作为这个位置的生成vector。</p>
</li>
<li><p>然后利用$z_i$和$h_i$ (decoder部分的hidden state) 计算 attention weight值$\alpha_{j,t-1}$</p>
</li>
<li><p>根据attention weight值，计算encoder的t时刻的output vector $c_t = \sum_{M}^{j=1} \alpha_{j,t-1} x_j$</p>
<p>$$\alpha_{j,t-1} = \frac{exp(z_j \cdot h_{t-1})}{\sum_{M}^{i=1} exp(z_i \cdot h_{t-1})}$$</p>
</li>
</ol>
<p>decoder部分采用了RNN和LSTM模型，output的计算方式为  $$h_t = g_{\Theta_1}(y_{t-1},h_{t-1},c_t)$$</p>
<p>这篇是比较接近目前最标准的seq2seq+attention的标准模型架构的。</p>
<p>性能结果（取k=10时的最好结果，即attention weight计算时选用k=10的卷积抽取方法，并且decoder为RNN）</p>
<ul>
<li>DUC-2004: <strong>Rouge-1</strong> 28.97  <strong>Rouge-2</strong> 8.26   <strong>Rouge-L</strong> 24.06</li>
<li>Gigaword:  <strong>Rouge-1</strong> 33.78  <strong>Rouge-2</strong> 15.97  <strong>Rouge-L</strong> 31.15</li>
</ul>
<p><br></p>
<h4 id="Abstractive-Text-Summarization-Using-Sequence-to-Sequence-RNNs-and-Beyond"><a href="#Abstractive-Text-Summarization-Using-Sequence-to-Sequence-RNNs-and-Beyond" class="headerlink" title="Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond"></a>Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1602.06023.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1602.06023.pdf</a></li>
<li>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos; IBM</li>
<li>CoNLL2016</li>
</ul>
<p>文章在seq2seq+attention的基础上提出了很多trick。</p>
<ul>
<li>large vocabulary trick(LVT)，提出在每个mini batch中decoder的词汇表受到encoder词汇表的限制，即decoder中的词汇表是由该batch中encoder词汇表中的高频词汇组成的，这样就减缓了docoder后的softmax层受到词汇表过大导致的计算瓶颈。</li>
<li>Feature Rich Encoder，本文提出增加一些额外的features，比如tags，TF，IDF，来融合到word embedding的matrix中。对于这些额外feature中的连续值，，转化分到各个类别块中，然后再用one-hot来表示。</li>
<li>Switching Generator-Pointer。考虑到很多中心词出现的次数很少或者不出现，而word embedding 对低频词很不友好，会出现unk来代表未出现或低频词的情况，但这会导致语意不通。所以本文提出了一种decoder/pointer机制来解决这个问题。模型中decoder带有一个开关，如果开关状态是打开generator，则生成一个单词；如果是关闭，decoder则生成一个原文单词位置的指针，然后拷贝到摘要中。pointer机制在解决低频词时鲁棒性比较强，因为使用了encoder中低频词的隐藏层表示作为输入，是一个上下文相关的表示，而不仅仅是一个词向量。</li>
<li>Hierarchical Attention。用sentence level和word level两层的attention来计算每个位置的attention。也就是说一个词的attention值既考虑sentence的位置也考虑word本身的位置</li>
</ul>
<p>性能结果</p>
<ul>
<li>Gigaword: <strong>Rouge-1</strong> 35.30 <strong>Rouge-2</strong> 16.64 <strong>Rouge-L</strong> 32.6</li>
</ul>
<ul>
<li>CNN/Daily Mail Corpus  <strong>Rouge-1</strong> 35.46 <strong>Rouge-2</strong> 13.30 <strong>Rouge-L</strong> 32.65</li>
</ul>
<p><br></p>
<h4 id="Selective-Encoding-for-Abstractive-Sentence-Summarization"><a href="#Selective-Encoding-for-Abstractive-Sentence-Summarization" class="headerlink" title="Selective Encoding for Abstractive Sentence Summarization"></a>Selective Encoding for Abstractive Sentence Summarization</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1704.07073.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.07073.pdf</a></li>
<li>Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou; MSRA&amp;HIT</li>
<li>ACL2017</li>
</ul>
<p><img src="/2019/02/13/扫文笔记2/7.png" width="80%" height="80%"></p>
<p>文章主要解决的是对一个句子的提取摘要，不是对整个文章，可以理解为将句子缩句。提出的模型主要分为三步进行处理。</p>
<ul>
<li>encoder部分通过双向RNN来构建对句子的representation</li>
<li>然后利用selective gate network根据第一步的结果构建第二级的representation</li>
<li>利用attention-equipped的decoder根据第二级的representation生成summary</li>
</ul>
<p>具体来说，先通过第一步，根据$x_i$得到一个二维的$h_i$（来自双向RNN），将$h_i$和用于表示句子的$s$ 一同输入selective fate来生成第二级的representation $h_i’$.其中$s$ 是将$h \leftarrow 1$和 $h \rightarrow n$ concat起来，表示句子的含义。</p>
<p>根据attention model的思想，从$h’$获取$c_t$。然后利用$c_{t-1}$,word embedding的$w_{t-1}$,$s_{t-1}$生成$s_t$, $s_0$是通过encode的hidden state初始化的。其中$s_t$和$h_t’$用于计算前面提到的attention weight。</p>
<p>然后再利用$w_{t-1}$,$c_t$,$s_t$来生成$r_t$, $r_t$用于softmax函数来生成最后的word。$r_t$是个两维的数，在用于softmax时，选用的是他其中比较大的值。</p>
<p>性能结果</p>
<ul>
<li>Gigaword: <strong>Rouge-1</strong> 46.86 <strong>Rouge-2</strong> 24.58 <strong>Rouge-L</strong> 43.53</li>
<li>DUC2004: <strong>Rouge-1</strong> 29.21 <strong>Rouge-2</strong> 9.56 <strong>Rouge-L</strong> 25.51</li>
</ul>
<p><br></p>
<h4 id="Global-Encoding-for-Abstractive-Summarization"><a href="#Global-Encoding-for-Abstractive-Summarization" class="headerlink" title="Global Encoding for Abstractive Summarization"></a>Global Encoding for Abstractive Summarization</h4><ul>
<li>文章地址 <a href="http://aclweb.org/anthology/P18-2027" target="_blank" rel="noopener">http://aclweb.org/anthology/P18-2027</a></li>
<li>Junyang Lin, Xu Sun, Shuming Ma, Qi Su; PKU</li>
<li><p>ACL 2018</p>
<p>文章主要解决ABS结果中的repeat问题。他认为ABS不同于机器翻译，不能直接单独套用attention，因为对齐上有问题。所以他提出的想法是在整个attention模型内增加卷积gate来获取全局信息，他通过不同核的CNN来获取不同n-gram的信息。另外还辅助以self-attention来获取整个全局信息。最后用一个sigmod函数来调节n-gram信息和全局信息的权重，整合所有的信息，弄出新的representation。【文章测试对比的是F score】</p>
</li>
</ul>
<p><br></p>
<h4 id="Controllable-Abstractive-Summarization"><a href="#Controllable-Abstractive-Summarization" class="headerlink" title="Controllable Abstractive Summarization"></a>Controllable Abstractive Summarization</h4><ul>
<li>文章地址 <a href="https://arxiv.org/pdf/1711.05217.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.05217.pdf</a></li>
<li>Angela Fan, David Grangier, Michael Auli, Facebook</li>
<li>ACL 2018</li>
</ul>
<p>文章主要解决的是个性化的文本摘要生成，比如支持不同长度的偏好，关键词的偏好，行文风格的偏好。文章的模型是基于Fackbook提出的Convolutional Sequence-to-Sequence。然后还加入了intra-attention的思想。主要的改动在于增加了input。以不同长度为例，在training时，先将长度分为几个size，然后每个范围的length长度都对应用一个special word来表示。在training时，生成对应长度的summary的input中就加上这个对应的special word来进行训练，最后test的时候，想要多长的，就加个这个length范围的special word组成最终的input。[但是具体这个word怎么加到input里面，文章中说的是直接加在input的最前面，emmm不是很理解为什么可以生效？]</p>
<p>因为是个性化文本生成摘要，所以rouge等数值其实没有很大的比对意义，但文中给出的结果还是state-of-the-art的。</p>
<p><br></p>
<h4 id="A-Deep-Reinforced-Model-For-Abstractive-Summarization"><a href="#A-Deep-Reinforced-Model-For-Abstractive-Summarization" class="headerlink" title="A Deep Reinforced Model For Abstractive Summarization"></a>A Deep Reinforced Model For Abstractive Summarization</h4><ul>
<li>文章地址 <a href="https://pdfs.semanticscholar.org/14a2/5c6008e1681715cc930f673f687418124d31.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/14a2/5c6008e1681715cc930f673f687418124d31.pdf</a></li>
<li>Romain Paulus, Caiming Xiong∗&amp; Richard Socher,  Salesforce</li>
<li>ICLR 2018</li>
</ul>
<p>文章主要解决的是ABS中在长文本 生成中容易产生的repeat问题。同时，作者考虑到，在训练中，我们是拿每次前一步的正确输出结果来作为下一步的输入的，而在test中，我们只能直接拿上一步的正确结果来作为input，两者之间，后者会有个错误叠加的问题，而前者则在训练中，并没有考虑这一步，而导致的结果准确性问题。</p>
<p>文章运用了不同于寻常的attention-encoder/decoder模型。</p>
<p><img src="/2019/02/13/扫文笔记2/8.png" width="90%" height="90%"></p>
<ul>
<li><p>Intra-Temporal Attention — 主要针对encoder部分</p>
<p>普通的Attention都是依次计算当前decoder的状态$s_t$与每个encoder的隐层状态$h_i$的得分$e_i$，然后归一化得到权重$α_i$，再加权求和得到上下文向量$c_t$。</p>
<p>这个attention则是先计算不同timestamp中decorder的隐层状态和hidden_input_state,并归一化，求得$e’_{ti}$,再对不同位置的encoder的hidden_input_state做归一化，然后计算得到上下文向量。【也就是上图第一个c】</p>
<p>$$e_{ti} =f(h^d_t,h^e_i)$$</p>
<p>$$f(h^d_t,h^e_i)=(h^d_t)^TW^e_{attn}h^e_i$$</p>
<p>$$e’_{ti} = \begin{cases} exp(e_{ti}) &amp; \text{ if } t=1 \\ \frac{exp(e_{ti})}{\sum_{j=1}^{t-1}exp(e_{ji})} &amp; \text{otherwise}  \end{cases}$$</p>
<p>$$\alpha^e_{ti} = \frac{e’_{ti}}{\sum_{j=1}^{n}e’_{tj}}$$</p>
<p>$$c^e_t=\sum_{i=1}^{n} \alpha^e_{ti}h^e_i$$</p>
</li>
<li><p>Intra-Decoder Attention — 主要针对decoder部分</p>
<p>这部分的attention是建立在decoder当前隐层状态和历史隐层状态的机制。根据当前timestamp的decoder隐层状态和前面所有timestamp的decoder隐层状态，归一化得到权重，再计算出decoder的上下文输出向量。【上图第二个c】</p>
<p><img src="/2019/02/13/扫文笔记2/9.png" width="90%" height="90%"></p>
</li>
<li><p>Generation/Pointer — 针对生成当前输出的部分</p>
<p>对于生成输出结果，这边选择用要么softmax生成，或者用point machine 拷贝罕见词。文章用概率来决定是用生成还是pointer。定义$u_t$为0/1，0表示生成，1表示pointer。</p>
<p>$$p(y_t|u_t = 0) = softmax(W_{out}[h^d_t || c^e_t || c^d_t ] + b_{out})$$</p>
<p>$$p(y_t=x_i|u_t=1)=\alpha^e_{ti}$$</p>
<p>$$p(u_t=1) = \sigma (W_u[h^d_t || c^e_t || c^d_t] + b_u)$$</p>
<p>$$p(y_t) = p(u_t = 1)p(y_t|u_t = 1) + p(u_t = 0)p(y_t|u_t = 0)$$</p>
<p>根据这个概率来算最后选哪种方法，然后training的时候，如果groud truth这里的输出是out of vocabulary的或者是pre-defined name entity,就让$u_t=1$</p>
</li>
</ul>
<ul>
<li>在beam search的时候，对于已经出现过的trigram，直接将概率设为0避免重复。</li>
</ul>
<p>在以前的训练模型中，我们常用Teacher Forcing算法。通常的目标函数都是负对数似然函数(NLL)，然而即便达到最优，在使用离散评价方式（ROUGE, CIDEr, BLEU）进行评价的时候，往往得不到最好的效果，主要原因有俩：1. 训练的时候使用正确的单词作为下一时间片的输入，每一步的错误不会累积，但是测试的时候每一步的错误会累积到下一时间片。2. 输出文本的词序是灵活的，离散的评价方式也考虑到了这种灵活性，但是NLL没有.</p>
<p>文章提供的训练方法是强化训练。直接去优化特定的离散矩阵。作者提出，每一步生成摘要词时，输出两个序列选择，一个是选择概率最大的词\widehat{y_t}，作为baseline，同时sample 得到词$y^s$，然后分别拿这两个与ground truth 的y 计算ROUGE指标评测值作为reward，然后再根据这个reward对模型进行奖励和惩罚，更新参数。</p>
<p><img src="/2019/02/13/扫文笔记2/10.png" width="60%" height="60%"></p>
<p>但是上述的目标函数不能保证生成的质量和可读性，所以最后作者选用将上述L_{rl}和maximum-likelihood training objective加权作为learning object function。</p>
<p><img src="/2019/02/13/扫文笔记2/11.png" width="40%" height="60%"></p>
<p><img src="/2019/02/13/扫文笔记2/12.png" width="40%" height="60%"></p>
<p>性能结果</p>
<ul>
<li>CNN/Daily Mail(only ML): <strong>Rouge-1</strong> 38.30 <strong>Rouge-2</strong> 14.81 <strong>Rouge-L</strong> 35.49</li>
<li>CNN/Daily Mail(only RL): <strong>Rouge-1</strong> 41.16 <strong>Rouge-2</strong> 15.75 <strong>Rouge-L</strong> <strong>39.08</strong></li>
<li>CNN/Daily Mail(ML+RL): <strong>Rouge-1</strong> 39.87 <strong>Rouge-2</strong> 15.82 <strong>Rouge-L</strong> 36.90        </li>
</ul>
<p>感想：</p>
<ul>
<li>这个直接beam search时跳过trigram合理么，bigram的话合理么。</li>
<li>强化学习看懂后再回来看这个地方的training。</li>
<li>还要看一下这个的代码，再理解一下他的intra-attention。</li>
<li>对于training时候提出的问题值得思考。</li>
<li>copy or generate的方法很多文章都选用了，可以深入推敲一下。</li>
</ul>
<p><br></p>
<h4 id="Controlling-Output-Length-in-Neural-Encoder-Decoders"><a href="#Controlling-Output-Length-in-Neural-Encoder-Decoders" class="headerlink" title="Controlling Output Length in Neural Encoder-Decoders"></a>Controlling Output Length in Neural Encoder-Decoders</h4><p>文章主要提出了四种方法来解决让Encoder-Decoder模型输出固定长度的ouput。</p>
<p>方法1和方法2都属于比较粗爆的方法，基本就是强行到长度了就停掉，给他换个EOS上去。如果没写到就不让停这样的操作手法2333。</p>
<p>方法三和四都是通过更改decoder部分来做的。</p>
<ul>
<li>三的做法是在decoder的每个timestamp上都新增一个关于长度的输入。这个输入的获取方式是通过本次output还剩余可以输出的长度乘以一个长度矩阵来获得的。【这个长度矩阵是通过training得到的】就通过这个增加input的方式，来解决输出问题。</li>
<li>四的做法和三有些类似，但是他只是用一个memory-cell来控制输出，主要方式是更改了decoder的初始化值。</li>
</ul>
<p>感想：</p>
<ul>
<li>前两个方法不需要考虑，后两个方法中，有一个问题是，对于同一个source，我想生成不同长度的结果，感觉不像是从头生成的句子，更像是从前面短小的地方加上一些词。就说明其实我们在用length控制的时候对前面词的输出或者说对整个意思的传达没有一个很好的影响。</li>
<li>对memory cell还有一定的不理解性。</li>
</ul>
<p>​    </p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/22/日本旅游/" rel="next" title="日本旅游攻略">
                <i class="fa fa-chevron-left"></i> 日本旅游攻略
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/15/NLP相关/" rel="prev" title="NLP各种相关模块">
                NLP各种相关模块 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
     <div id="gitalk-container"></div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yunyan.hong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summariztion"><span class="nav-number">1.</span> <span class="nav-text">A Neural Attention Model for Abstractive Sentence Summariztion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Abstractive-Sentence-Summarization-with-Attentive-Recurrent-Neural-Networks"><span class="nav-number">2.</span> <span class="nav-text">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Abstractive-Text-Summarization-Using-Sequence-to-Sequence-RNNs-and-Beyond"><span class="nav-number">3.</span> <span class="nav-text">Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selective-Encoding-for-Abstractive-Sentence-Summarization"><span class="nav-number">4.</span> <span class="nav-text">Selective Encoding for Abstractive Sentence Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Global-Encoding-for-Abstractive-Summarization"><span class="nav-number">5.</span> <span class="nav-text">Global Encoding for Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Controllable-Abstractive-Summarization"><span class="nav-number">6.</span> <span class="nav-text">Controllable Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Deep-Reinforced-Model-For-Abstractive-Summarization"><span class="nav-number">7.</span> <span class="nav-text">A Deep Reinforced Model For Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Controlling-Output-Length-in-Neural-Encoder-Decoders"><span class="nav-number">8.</span> <span class="nav-text">Controlling Output Length in Neural Encoder-Decoders</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yunyan.hong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '214b2aee446518ccee23',
          clientSecret: '6b5e9007abf73fbcdce7d46cc4ae66a5fa5e6a3d',
          repo: 'hongyunyan.github.io',
          owner: 'hongyunyan',
          admin: ['hongyunyan'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
