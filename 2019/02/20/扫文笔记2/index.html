<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="A Neural Attention Model for Abstractive Sentence Summariztion 文章地址：https://www.aclweb.org/anthology/D/D15/D15-1044.pdf   Alexander M. Rush et al., Facebook AI Research/Harvard EMNLP2015  文章利用Seq2Seq模">
<meta property="og:type" content="article">
<meta property="og:title" content="Abstractive Summarization 扫文清单">
<meta property="og:url" content="http://yoursite.com/2019/02/20/扫文笔记2/index.html">
<meta property="og:site_name" content="Yunyan Hong">
<meta property="og:description" content="A Neural Attention Model for Abstractive Sentence Summariztion 文章地址：https://www.aclweb.org/anthology/D/D15/D15-1044.pdf   Alexander M. Rush et al., Facebook AI Research/Harvard EMNLP2015  文章利用Seq2Seq模">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/1.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/2.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/3.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/4.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/5.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/6.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/7.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/8.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/9.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/10.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/11.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/12.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/13.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/14.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/15.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/16.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/17.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/18.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/19.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/20.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/21.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/22.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/23.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/24.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/25.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/26.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/27.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/28.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/29.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/30.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/31.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/32.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/33.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/34.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/35.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/36.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/37.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/38.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/39.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/40.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/41.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/42.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/43.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/44.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/45.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/46.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/47.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/48.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/49.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/50.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/51.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/52.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/53.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/54.png">
<meta property="og:image" content="http://yoursite.com/2019/02/20/扫文笔记2/55.png">
<meta property="og:updated_time" content="2019-04-02T06:40:26.110Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Abstractive Summarization 扫文清单">
<meta name="twitter:description" content="A Neural Attention Model for Abstractive Sentence Summariztion 文章地址：https://www.aclweb.org/anthology/D/D15/D15-1044.pdf   Alexander M. Rush et al., Facebook AI Research/Harvard EMNLP2015  文章利用Seq2Seq模">
<meta name="twitter:image" content="http://yoursite.com/2019/02/20/扫文笔记2/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/02/20/扫文笔记2/">





  <title>Abstractive Summarization 扫文清单 | Yunyan Hong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yunyan Hong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/02/20/扫文笔记2/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yunyan.hong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yunyan Hong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Abstractive Summarization 扫文清单</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-02-20T13:53:26+08:00">
                2019-02-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h4 id="A-Neural-Attention-Model-for-Abstractive-Sentence-Summariztion"><a href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summariztion" class="headerlink" title="A Neural Attention Model for Abstractive Sentence Summariztion"></a>A Neural Attention Model for Abstractive Sentence Summariztion</h4><ul>
<li>文章地址：<a href="https://www.aclweb.org/anthology/D/D15/D15-1044.pdf" target="_blank" rel="noopener">https://www.aclweb.org/anthology/D/D15/D15-1044.pdf</a></li>
</ul>
<ul>
<li>Alexander M. Rush et al., Facebook AI Research/Harvard</li>
<li>EMNLP2015</li>
</ul>
<p>文章利用Seq2Seq模型来做abstractive summarization。提出三种encoder模式和decoder模式。文章整个模型框架以及主体公式如下图</p>
<p><img src="/2019/02/20/扫文笔记2/1.png" width="50%" height="50%"></p>
<p><img src="/2019/02/20/扫文笔记2/2.png" width="50%" height="50%"></p>
<p>对于左图的encoder部分，作者提出了三种encoder的方法，分别为</p>
<ol>
<li><p>Bag-of-Words Encoder</p>
<p>这个encoder赋予每个词相同的权重，忽略了词序问题。整个encoder中只有F (word Emedding Matrix) 是parameter</p>
<p><img src="/2019/02/20/扫文笔记2/3.png" width="50%" height="50%"></p>
</li>
<li><p>Convolutional Encoder</p>
<p>F是Word Embedding Matrix，Q是卷积核，可以理解为卷积模型。整体考虑的是邻近单词的互动</p>
<p><img src="/2019/02/20/扫文笔记2/4.png" width="50%" height="50%"></p>
</li>
<li><p>Attention-Based Encoder</p>
<p>整体和BoW类似，只是把Uniform分布的部分换成了input和summary之间的soft alignment值，即上下文能对齐的part提供比较高的权重（来自机器翻译的思想）</p>
<p><img src="/2019/02/20/扫文笔记2/5.png" width="50%" height="50%"></p>
</li>
</ol>
<p>对于Generation部分，文章采用Beam Search的方法。即给定一个beam size K，在生成每一个summary word时，都保留概率最大的K个词。直到生成的词是EOS或者达到最大句子长度限制。最后得到的结果是K个最好的sentence summary。</p>
<p><img src="/2019/02/20/扫文笔记2/6.png" width="50%" height="50%"></p>
<p>性能结果（整个架构模型不加上trick部分）</p>
<ul>
<li>DUC-2004: <strong>Rouge-1</strong> 26.55  <strong>Rouge-2</strong> 7.06  <strong>Rouge-L</strong> 22.05</li>
<li>Gigaword:  <strong>Rouge-1</strong> 30.88 <strong>Rouge-2</strong> 12.65 <strong>Rouge-L</strong> 28.34</li>
</ul>
<p><br></p>
<h4 id="Abstractive-Sentence-Summarization-with-Attentive-Recurrent-Neural-Networks"><a href="#Abstractive-Sentence-Summarization-with-Attentive-Recurrent-Neural-Networks" class="headerlink" title="Abstractive Sentence Summarization with Attentive Recurrent Neural Networks"></a>Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</h4><ul>
<li>文章地址：<a href="http://www.aclweb.org/anthology/N16-1012" target="_blank" rel="noopener">http://www.aclweb.org/anthology/N16-1012</a></li>
</ul>
<ul>
<li>Sumit Chopra et al., Facebook AI Research【和上一篇是姊妹篇】</li>
<li>NAACL2016</li>
</ul>
<p>文章模型的感觉是利用一个attentive的encoder进行encoder，然后正常的rnn进行decoder。用现在的attention model模型来看的意思，这边的$k!=v$。即创造attention vector的input部分并不是encoder中真正的encoder（就是不是纯的每个位置的word embedding vector），而是</p>
<ol>
<li><p>先将每个位置xi的word embedding vector和表示位置信息的vector进行相加，作为每个位置词对应的vector，记为$a_i ( 1\times d) $</p>
</li>
<li><p>然后利用一个一维卷积核做一个卷积操作，就k个$ B_{d \times q} $矩阵, $z_{ik} = \sum a_i \cdot B^k$, $z_i = [z_1,z_2,…,z_k]$, $z_i$作为这个位置的生成vector。</p>
</li>
<li><p>然后利用$z_i$和$h_i$ (decoder部分的hidden state) 计算 attention weight值$\alpha_{j,t-1}$</p>
</li>
<li><p>根据attention weight值，计算encoder的t时刻的output vector $c_t = \sum_{M}^{j=1} \alpha_{j,t-1} x_j$</p>
<p>$$\alpha_{j,t-1} = \frac{exp(z_j \cdot h_{t-1})}{\sum_{M}^{i=1} exp(z_i \cdot h_{t-1})}$$</p>
</li>
</ol>
<p>decoder部分采用了RNN和LSTM模型，output的计算方式为  $$h_t = g_{\Theta_1}(y_{t-1},h_{t-1},c_t)$$</p>
<p>这篇是比较接近目前最标准的seq2seq+attention的标准模型架构的。</p>
<p>性能结果（取k=10时的最好结果，即attention weight计算时选用k=10的卷积抽取方法，并且decoder为RNN）</p>
<ul>
<li>DUC-2004: <strong>Rouge-1</strong> 28.97  <strong>Rouge-2</strong> 8.26   <strong>Rouge-L</strong> 24.06</li>
<li>Gigaword:  <strong>Rouge-1</strong> 33.78  <strong>Rouge-2</strong> 15.97  <strong>Rouge-L</strong> 31.15</li>
</ul>
<p><br></p>
<h4 id="Abstractive-Text-Summarization-Using-Sequence-to-Sequence-RNNs-and-Beyond"><a href="#Abstractive-Text-Summarization-Using-Sequence-to-Sequence-RNNs-and-Beyond" class="headerlink" title="Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond"></a>Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1602.06023.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1602.06023.pdf</a></li>
<li>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos; IBM</li>
<li>CoNLL2016</li>
</ul>
<p>文章在seq2seq+attention的基础上提出了很多trick。</p>
<ul>
<li>large vocabulary trick(LVT)，提出在每个mini batch中decoder的词汇表受到encoder词汇表的限制，即decoder中的词汇表是由该batch中encoder词汇表中的高频词汇组成的，这样就减缓了docoder后的softmax层受到词汇表过大导致的计算瓶颈。</li>
<li>Feature Rich Encoder，本文提出增加一些额外的features，比如tags，TF，IDF，来融合到word embedding的matrix中。对于这些额外feature中的连续值，，转化分到各个类别块中，然后再用one-hot来表示。</li>
<li>Switching Generator-Pointer。考虑到很多中心词出现的次数很少或者不出现，而word embedding 对低频词很不友好，会出现unk来代表未出现或低频词的情况，但这会导致语意不通。所以本文提出了一种decoder/pointer机制来解决这个问题。模型中decoder带有一个开关，如果开关状态是打开generator，则生成一个单词；如果是关闭，decoder则生成一个原文单词位置的指针，然后拷贝到摘要中。pointer机制在解决低频词时鲁棒性比较强，因为使用了encoder中低频词的隐藏层表示作为输入，是一个上下文相关的表示，而不仅仅是一个词向量。</li>
<li>Hierarchical Attention。用sentence level和word level两层的attention来计算每个位置的attention。也就是说一个词的attention值既考虑sentence的位置也考虑word本身的位置</li>
</ul>
<p>性能结果</p>
<ul>
<li>Gigaword: <strong>Rouge-1</strong> 35.30 <strong>Rouge-2</strong> 16.64 <strong>Rouge-L</strong> 32.6</li>
</ul>
<ul>
<li>CNN/Daily Mail Corpus  <strong>Rouge-1</strong> 35.46 <strong>Rouge-2</strong> 13.30 <strong>Rouge-L</strong> 32.65</li>
</ul>
<p><br></p>
<h4 id="Selective-Encoding-for-Abstractive-Sentence-Summarization"><a href="#Selective-Encoding-for-Abstractive-Sentence-Summarization" class="headerlink" title="Selective Encoding for Abstractive Sentence Summarization"></a>Selective Encoding for Abstractive Sentence Summarization</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1704.07073.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1704.07073.pdf</a></li>
<li>Qingyu Zhou, Nan Yang, Furu Wei, Ming Zhou; MSRA&amp;HIT</li>
<li>ACL2017</li>
</ul>
<p><img src="/2019/02/20/扫文笔记2/7.png" width="80%" height="80%"></p>
<p>文章主要解决的是对一个句子的提取摘要，不是对整个文章，可以理解为将句子缩句。提出的模型主要分为三步进行处理。</p>
<ul>
<li>encoder部分通过双向RNN来构建对句子的representation</li>
<li>然后利用selective gate network根据第一步的结果构建第二级的representation</li>
<li>利用attention-equipped的decoder根据第二级的representation生成summary</li>
</ul>
<p>具体来说，先通过第一步，根据$x_i$得到一个二维的$h_i$（来自双向RNN），将$h_i$和用于表示句子的$s$ 一同输入selective fate来生成第二级的representation $h_i’$.其中$s$ 是将$h \leftarrow 1$和 $h \rightarrow n$ concat起来，表示句子的含义。</p>
<p>根据attention model的思想，从$h’$获取$c_t$。然后利用$c_{t-1}$,word embedding的$w_{t-1}$,$s_{t-1}$生成$s_t$, $s_0$是通过encode的hidden state初始化的。其中$s_t$和$h_t’$用于计算前面提到的attention weight。</p>
<p>然后再利用$w_{t-1}$,$c_t$,$s_t$来生成$r_t$, $r_t$用于softmax函数来生成最后的word。$r_t$是个两维的数，在用于softmax时，选用的是他其中比较大的值。</p>
<p>性能结果</p>
<ul>
<li>Gigaword: <strong>Rouge-1</strong> 46.86 <strong>Rouge-2</strong> 24.58 <strong>Rouge-L</strong> 43.53</li>
<li>DUC2004: <strong>Rouge-1</strong> 29.21 <strong>Rouge-2</strong> 9.56 <strong>Rouge-L</strong> 25.51</li>
</ul>
<p><br></p>
<h4 id="Global-Encoding-for-Abstractive-Summarization"><a href="#Global-Encoding-for-Abstractive-Summarization" class="headerlink" title="Global Encoding for Abstractive Summarization"></a>Global Encoding for Abstractive Summarization</h4><ul>
<li>文章地址 <a href="http://aclweb.org/anthology/P18-2027" target="_blank" rel="noopener">http://aclweb.org/anthology/P18-2027</a></li>
<li>Junyang Lin, Xu Sun, Shuming Ma, Qi Su; PKU</li>
<li><p>ACL 2018</p>
<p>文章主要解决ABS结果中的repeat问题。他认为ABS不同于机器翻译，不能直接单独套用attention，因为对齐上有问题。所以他提出的想法是在整个attention模型内增加卷积gate来获取全局信息，他通过不同核的CNN来获取不同n-gram的信息。另外还辅助以self-attention来获取整个全局信息。最后用一个sigmod函数来调节n-gram信息和全局信息的权重，整合所有的信息，弄出新的representation。【文章测试对比的是F score】</p>
</li>
</ul>
<p><br></p>
<h4 id="Controllable-Abstractive-Summarization"><a href="#Controllable-Abstractive-Summarization" class="headerlink" title="Controllable Abstractive Summarization"></a>Controllable Abstractive Summarization</h4><ul>
<li>文章地址 <a href="https://arxiv.org/pdf/1711.05217.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.05217.pdf</a></li>
<li>Angela Fan, David Grangier, Michael Auli, Facebook</li>
<li>ACL 2018</li>
</ul>
<p>文章主要解决的是个性化的文本摘要生成，比如支持不同长度的偏好，关键词的偏好，行文风格的偏好。文章的模型是基于Fackbook提出的Convolutional Sequence-to-Sequence。然后还加入了intra-attention的思想。主要的改动在于增加了input。以不同长度为例，在training时，先将长度分为几个size，然后每个范围的length长度都对应用一个special word来表示。在training时，生成对应长度的summary的input中就加上这个对应的special word来进行训练，最后test的时候，想要多长的，就加个这个length范围的special word组成最终的input。[但是具体这个word怎么加到input里面，文章中说的是直接加在input的最前面，emmm不是很理解为什么可以生效？]</p>
<p>因为是个性化文本生成摘要，所以rouge等数值其实没有很大的比对意义，但文中给出的结果还是state-of-the-art的。</p>
<p><br></p>
<h4 id="A-Deep-Reinforced-Model-For-Abstractive-Summarization"><a href="#A-Deep-Reinforced-Model-For-Abstractive-Summarization" class="headerlink" title="A Deep Reinforced Model For Abstractive Summarization"></a>A Deep Reinforced Model For Abstractive Summarization</h4><ul>
<li>文章地址 <a href="https://pdfs.semanticscholar.org/14a2/5c6008e1681715cc930f673f687418124d31.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/14a2/5c6008e1681715cc930f673f687418124d31.pdf</a></li>
<li>Romain Paulus, Caiming Xiong∗&amp; Richard Socher,  Salesforce</li>
<li>ICLR 2018</li>
</ul>
<p>文章主要解决的是ABS中在长文本 生成中容易产生的repeat问题。同时，作者考虑到，在训练中，我们是拿每次前一步的正确输出结果来作为下一步的输入的，而在test中，我们只能直接拿上一步的正确结果来作为input，两者之间，后者会有个错误叠加的问题，而前者则在训练中，并没有考虑这一步，而导致的结果准确性问题。</p>
<p>文章运用了不同于寻常的attention-encoder/decoder模型。</p>
<p><img src="/2019/02/20/扫文笔记2/8.png" width="90%" height="90%"></p>
<ul>
<li><p>Intra-Temporal Attention — 主要针对encoder部分</p>
<p>普通的Attention都是依次计算当前decoder的状态$s_t$与每个encoder的隐层状态$h_i$的得分$e_i$，然后归一化得到权重$α_i$，再加权求和得到上下文向量$c_t$。</p>
<p>这个attention则是先计算不同timestamp中decorder的隐层状态和hidden_input_state,并归一化，求得$e’_{ti}$,再对不同位置的encoder的hidden_input_state做归一化，然后计算得到上下文向量。【也就是上图第一个c】</p>
<p>$$e_{ti} =f(h^d_t,h^e_i)$$</p>
<p>$$f(h^d_t,h^e_i)=(h^d_t)^TW^e_{attn}h^e_i$$</p>
<p>$$e’_{ti} = \begin{cases} exp(e_{ti}) &amp; \text{ if } t=1 \\ \frac{exp(e_{ti})}{\sum_{j=1}^{t-1}exp(e_{ji})} &amp; \text{otherwise}  \end{cases}$$</p>
<p>$$\alpha^e_{ti} = \frac{e’_{ti}}{\sum_{j=1}^{n}e’_{tj}}$$</p>
<p>$$c^e_t=\sum_{i=1}^{n} \alpha^e_{ti}h^e_i$$</p>
</li>
<li><p>Intra-Decoder Attention — 主要针对decoder部分</p>
<p>这部分的attention是建立在decoder当前隐层状态和历史隐层状态的机制。根据当前timestamp的decoder隐层状态和前面所有timestamp的decoder隐层状态，归一化得到权重，再计算出decoder的上下文输出向量。【上图第二个c】</p>
<p><img src="/2019/02/20/扫文笔记2/9.png" width="90%" height="90%"></p>
</li>
<li><p>Generation/Pointer — 针对生成当前输出的部分</p>
<p>对于生成输出结果，这边选择用要么softmax生成，或者用point machine 拷贝罕见词。文章用概率来决定是用生成还是pointer。定义$u_t$为0/1，0表示生成，1表示pointer。</p>
<p>$$p(y_t|u_t = 0) = softmax(W_{out}[h^d_t || c^e_t || c^d_t ] + b_{out})$$</p>
<p>$$p(y_t=x_i|u_t=1)=\alpha^e_{ti}$$</p>
<p>$$p(u_t=1) = \sigma (W_u[h^d_t || c^e_t || c^d_t] + b_u)$$</p>
<p>$$p(y_t) = p(u_t = 1)p(y_t|u_t = 1) + p(u_t = 0)p(y_t|u_t = 0)$$</p>
<p>根据这个概率来算最后选哪种方法，然后training的时候，如果groud truth这里的输出是out of vocabulary的或者是pre-defined name entity,就让$u_t=1$</p>
</li>
</ul>
<ul>
<li>在beam search的时候，对于已经出现过的trigram，直接将概率设为0避免重复。</li>
</ul>
<p>在以前的训练模型中，我们常用Teacher Forcing算法。通常的目标函数都是负对数似然函数(NLL)，然而即便达到最优，在使用离散评价方式（ROUGE, CIDEr, BLEU）进行评价的时候，往往得不到最好的效果，主要原因有俩：1. 训练的时候使用正确的单词作为下一时间片的输入，每一步的错误不会累积，但是测试的时候每一步的错误会累积到下一时间片。2. 输出文本的词序是灵活的，离散的评价方式也考虑到了这种灵活性，但是NLL没有.</p>
<p>文章提供的训练方法是强化训练。直接去优化特定的离散矩阵。作者提出，每一步生成摘要词时，输出两个序列选择，一个是选择概率最大的词$\widehat{y_t}$，作为baseline，同时sample 得到词$y^s$，然后分别拿这两个与ground truth 的y 计算ROUGE指标评测值作为reward，然后再根据这个reward对模型进行奖励和惩罚，更新参数。</p>
<p><img src="/2019/02/20/扫文笔记2/10.png" width="60%" height="60%"></p>
<p>但是上述的目标函数不能保证生成的质量和可读性，所以最后作者选用将上述$L_{rl}$和maximum-likelihood training objective加权作为learning object function。</p>
<p><img src="/2019/02/20/扫文笔记2/11.png" width="40%" height="60%"></p>
<p><img src="/2019/02/20/扫文笔记2/12.png" width="40%" height="60%"></p>
<p>性能结果</p>
<ul>
<li>CNN/Daily Mail(only ML): <strong>Rouge-1</strong> 38.30 <strong>Rouge-2</strong> 14.81 <strong>Rouge-L</strong> 35.49</li>
<li>CNN/Daily Mail(only RL): <strong>Rouge-1</strong> 41.16 <strong>Rouge-2</strong> 15.75 <strong>Rouge-L</strong> <strong>39.08</strong><ul>
<li>CNN/Daily Mail(ML+RL): <strong>Rouge-1</strong> 39.87 <strong>Rouge-2</strong> 15.82 <strong>Rouge-L</strong> 36.90</li>
</ul>
</li>
</ul>
<p>感想：</p>
<ul>
<li>这个直接beam search时跳过trigram合理么，bigram的话合理么。</li>
<li>强化学习看懂后再回来看这个地方的training。</li>
<li>还要看一下这个的代码，再理解一下他的intra-attention。</li>
<li>对于training时候提出的问题值得思考。</li>
<li>copy or generate的方法很多文章都选用了，可以深入推敲一下。</li>
</ul>
<p><br></p>
<h4 id="Controlling-Output-Length-in-Neural-Encoder-Decoders"><a href="#Controlling-Output-Length-in-Neural-Encoder-Decoders" class="headerlink" title="Controlling Output Length in Neural Encoder-Decoders"></a>Controlling Output Length in Neural Encoder-Decoders</h4><ul>
<li>文章地址 <a href="https://arxiv.org/pdf/1609.09552.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.09552.pdf</a></li>
<li>Yuta Kikuchi, Graham Neubig, Ryohei Sasano, Hiroya Takamura, Manabu Okumura,Tokyo Institute of Technology, Carnegie Mellon University</li>
<li>EMNLP 2016</li>
</ul>
<p>文章主要提出了四种方法来解决让Encoder-Decoder模型输出固定长度的ouput。</p>
<p>方法1和方法2都属于比较粗爆的方法，基本就是强行到长度了就停掉，给他换个EOS上去。如果没写到就不让停这样的操作手法2333。</p>
<p>方法三和四都是通过更改decoder部分来做的。</p>
<ul>
<li>三的做法是在decoder的每个timestamp上都新增一个关于长度的输入。这个输入的获取方式是通过本次output还剩余可以输出的长度乘以一个长度矩阵来获得的。【这个长度矩阵是通过training得到的】就通过这个增加input的方式，来解决输出问题。</li>
<li>四的做法和三有些类似，但是他只是用一个memory-cell来控制输出，主要方式是更改了decoder的初始化值。</li>
</ul>
<p>感想：</p>
<ul>
<li>前两个方法不需要考虑，后两个方法中，有一个问题是，对于同一个source，我想生成不同长度的结果，感觉不像是从头生成的句子，更像是从前面短小的地方加上一些词。就说明其实我们在用length控制的时候对前面词的输出或者说对整个意思的传达没有一个很好的影响。</li>
<li>对memory cell还有一定的不理解性。</li>
</ul>
<p><br></p>
<h4 id="Deep-contextualized-word-representations"><a href="#Deep-contextualized-word-representations" class="headerlink" title="Deep contextualized word representations"></a>Deep contextualized word representations</h4><ul>
<li>文章地址 <a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1802.05365.pdf</a></li>
<li>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner… Allen Institute for Artificial Intelligence</li>
<li>NAACL 2018  Best Paper</li>
</ul>
<p>文章主要提出了一种新的embedding的方式，对应于传统的静态embedding，EMLo方式只训练language model，而word embedding是需要通过输入的句子上下文输出的。即很大程度上解决了一词多义的embedding问题。</p>
<p>首先模型运用的是双向的language model[Bidirectional language models(BiLM)]。即给定N个tokens的序列$(t_1, t_2, … , t_n)$,     前向language model就是通过前k-1个输入序列 $(t_1, t_2, …,t_k)$的hidden表示, 预测第$k$个位置的token, 反向的language model就是给定后面的序列, 预测之前的, 然后将language model的第$k$个位置的hidden输出做为word embedding.</p>
<p>BiLM训练的目标是最大化下图公式。</p>
<p><img src="/2019/02/20/扫文笔记2/13.png" width="90%" height="90%"></p>
<p>模型用的是L层双向LSTM，最原始的word embedding表示中，第k位置的词我们用$x_k^{LM}$表示，然后每一层的LSTM都会输出一个context-dependent的表示,正向的为$ \underset{h}{\rightarrow}^{LM}_{k,j}$,反向的为$\underset{h}{\leftarrow}^{LM}_{k,j}$, $j=1,2,…L$,其中$ \underset{h}{\rightarrow}^{LM}_{k,L}$是用来结合softmax一起预测$t_{k+1}$的。</p>
<p>对于L层的biLM的输出，我们用$R_k =\left \{  h^{LM}_{k,j} | j = 0,…L \right \}$表示。其中$h^{LM}_{k,j} =\left [  \underset{h}{\rightarrow}^{LM}<em>{k,j} ; \underset{h}{\leftarrow}^{LM}\</em>{k,j}\right ]$。</p>
<p>然后ELMo把R中所有层的表示一起压缩成一个vector。$ELMo_k = E(R_k ; \Theta_e )$。最简单的方式是直接ELMo取BiLM中最top层的输出表示，即$E(R_k) = h^{LM}_{k,j}$。更通用的是对于每一层的表示，我们计算task special的权重$$ELMo_k^{task} = E(R_k; \Theta^{task})  = \gamma^{task} \sum_{j=0}^{L}s_j^{task}h^{LM}_{k,j}$$，其中$s^{task}$是softmax-normalized weights。</p>
<p>如何使用ELMo：[整体流程是：训练好一个lm模型，然后输入新任务中的上下文，获得对应的层层embedding，然后用到具体任务中。然后后续调整每一层的权重？]</p>
<ul>
<li>将该词向量加到一般的任务的方法是, 先固定一个权重, 构成一个$EMLo^{task}$ , 然后联结这个vector和最初始词向量$x$, 然后将这个向量输入到任务的模型中去. 然后在训练中一同训练权重因子.</li>
<li>我们可以在output处也加一个这样的向量,即$\left [ h_k; ELMo^{task}_k \right ]$ , 不同是,这里应该采用与输入的向量用不同的权重因子.</li>
<li>在ELMo模型中加入dropout，并用L2 loss提升模型。$\lambda$越大, 越有取各层平均值的意思, 越小, 越可以发挥各个层之间的不同带来的效果. 但是并不是说, 越小越好。</li>
</ul>
<p>感想：</p>
<ul>
<li>底层对语义的表达好坏从根本上影响结果。</li>
</ul>
<p><br></p>
<h4 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.03762.pdf</a></li>
<li>Google AI</li>
<li>arXiv 2017</li>
</ul>
<p>模型介绍：</p>
<p>整个transformer的架构图如下【文章中N取6】：</p>
<p><img src="/2019/02/20/扫文笔记2/14.png" width="50%" height="90%"></p>
<p>左边部分为encoder，右边为decoder。</p>
<p>首先对于输入而言，因为整个模型需要一个地方捕捉到在文本中单词的顺序问题，因此在开始的时候对于普通的embedding上，我们加上了positional encoding。这边的positional encoding，我们采用了很简单的一个sin和cos函数。并且经过计算测试，这种简单的公式得到的结果和learn得到的position表示效果极嗯相同，因此就选择这种表达方式，因为这种方式对于文本的length没有要求。【这边postion encoding得到的embedding和普通的embedding 维数相同，因此就直接相加即可】</p>
<p><img src="/2019/02/20/扫文笔记2/15.png" width="50%"></p>
<p>下面我们看一下具体的每一层的内容。encoder的每一层一共包括了两个sub-layers，一个是multi-head attention，一个是feed forward。</p>
<ul>
<li><p>multi-head attention</p>
<p>首先这边选用的是self-attention。首先根据embedding的每个单词的输入向量生成三个向量(Q,K,V)。因为是self-attention，所以Q=K=V。$Q=XW^Q， K=XW^K，V=XW^V$。self attention就是对句子中的每个词之间做点积（或者其他公式的相似性计算)，文中对这个值再除以了Q向量维度的平方根来使梯度更稳定，然后用softmax归一化。再用softmax获得的值乘以对应的向量相加，获得对应位置的输出。用矩阵的方式表示就是</p>
<p><img src="/2019/02/20/扫文笔记2/16.png" width="50%"></p>
<p>然后multi-head的做法就是用多个$W_Q, W_K, W_V$【transformer用的是8个】这些都是随机初始化后训练的，表示的是输入词向量投射到不同表示的字空间中。通过8组不同的W值生成的attention矩阵，拼接起来，乘以一个表示权重的$W_0$,获得最后的输出表示。</p>
<p>相关参考blog地址：<a href="https://zhuanlan.zhihu.com/p/54356280" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54356280</a></p>
</li>
<li><p>feed forward</p>
<p>也就是前馈神经网络。各神经元分层排列，每个神经元只与前一层的神经元相连。接收前一层的输出，并输出给下一层．各层间没有反馈。其实就是一个MLP。</p>
<p><img src="/2019/02/20/扫文笔记2/17.png" width="50%"></p>
</li>
</ul>
<ul>
<li><p>residual connections</p>
<p>multi-head attention输出和feed forward输出都先通过residual connections处理。也就是输入和输出求和并归一化【应该就是一横排归一？】</p>
</li>
</ul>
<p>decoder每一层含有三个sub-layers。除了encoder中的那两个sub-layer，还有一个masked multi-head attention，这个attention也是self attention，但是它要消除右侧单词对当前单词的影响【把右侧的词语设为负无穷】，也就是说只和已经生成的左侧词语进行attention。然后decoder中的multi head attention不是self-attention，encoder的输出为key和value，下层的输出为query。其余的和encoder基本相同。</p>
<p>感想and疑惑：</p>
<ul>
<li>举个例子，the animal didn’t cross the street because it was too tired 在multi-head attention训练下，这个it真的能训练出来输出的embedding和animal差不多吗？如何做到的啊？</li>
<li>这个模式最早是用在mt的，但是mt的确词词应当对应，但abs并不存在的，如果修改，更新呢？</li>
<li>相似度算法除了词的vector的距离还有什么其他优秀的方式，比较的不是相似度？</li>
<li>为啥head数量会很大影响特征抽取效果，其实还没有深刻理解head这种操作的玄机。</li>
</ul>
<p><br></p>
<h4 id="A-Reinforced-Topic-Aware-Convolutional-Sequence-to-Sequence-Model-for-Abstractive-Text-Summarization"><a href="#A-Reinforced-Topic-Aware-Convolutional-Sequence-to-Sequence-Model-for-Abstractive-Text-Summarization" class="headerlink" title="A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization"></a>A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization</h4><ul>
<li>文章地址<a href="https://www.ijcai.org/proceedings/2018/0619.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2018/0619.pdf</a></li>
<li>Tencent Data Center of SNG, ETH Zurich, Columbia University, Tencent AI Lab</li>
<li>IJCAI-ECAI 2018</li>
</ul>
<p>文章在ConvSeq2Seq模型中引入了topic 模型，如下图所示。</p>
<p><img src="/2019/02/20/扫文笔记2/18.png" width="60%"></p>
<p>这边的topic模型用的是LDA。Topic embedding的方式为：首先利用LDA训练出topic vocabulary【取的是N个topic最高可能性的word】，这个vocabulary是被包含于整个text的vocabulary的。对于每个$x_i$,如果这个词不属于topic vocabulary，则直接用原始的word embeding值作为他的topic embedding，如果属于的话，则用他对应的topic embedding matrix中的一个vector，这个matrix是通过对一个pretrain的topic matrix【根据LDA】normalization，即对于每一个row除以对应word被topic选中的次数。</p>
<p>convS2S部分和原facebook AI出得文章是一致的。</p>
<p>对于两个encoder部分，我们在biased probablitiy generation生成的部分进行合作。如果这一个timestamp生成的候选word在topic vocabulary中，我们就将两个encoder部分一定方法相加，如果候选word不在topic，则只取非topic部分的信息。</p>
<p>然后就是在训练中用的是强化训练【感觉好像在生成问题中，越来越多选用的都是强化学习，因为考虑到错误的累积性问题】</p>
<p><br></p>
<h4 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a>Get To The Point: Summarization with Pointer-Generator Networks</h4><ul>
<li>文章地址：<a href="https://nlp.stanford.edu/pubs/see2017get.pdf" target="_blank" rel="noopener">https://nlp.stanford.edu/pubs/see2017get.pdf</a></li>
<li>Stanford， Google Brain</li>
<li>ACL 2017</li>
</ul>
<p>文章提出seq2seq+attention的传统模型的两个shortcuts：</p>
<ul>
<li>模型容易不准确地再现事实细节，也就是说模型生成的摘要不准确；</li>
<li>生成的摘要往往会重复，也就是会重复生成一些词或者句子。[主要是因为 decoder 过程太过依赖于上一个单词，而不是 longer-term 的信息，所以一个重复的单词会 trigger 出死循环]</li>
</ul>
<p>因此作者提出了利用pointer-generator network以及converge来实现summarization。</p>
<p><img src="/2019/02/20/扫文笔记2/19.png" width="100%"></p>
<p>pointer-generator network提出结合softmax生成词和pointer选用input的词语。$p_{gen}$是用来计算是从input中复制个词语还是从词汇表生成词语的概率。$P_{vocab}(w)$用于表示softmax层算出来生成这个词语的概率，最后那个part就算attention权重计算的每个input位置的词的权重。如果这个词语并没有在input中，则后半个式子为0，如果词语是OOV，则前半个式子为0。</p>
<p><img src="/2019/02/20/扫文笔记2/20.png" width="50%"></p>
<p>Converage机制用于解决重复生成的问题。$a^t_i$表示的是每个词被注意的程度，我们希望所有的地方都被注意到，而不是在每次预测时候注意类似的地方，这样容易生成相同的内容，因此作者提出调整attention中e值计算方式。</p>
<p>原先的计算方式为$e^t_i = v^Ttanh(W_hh_i + W_ss_t + b_attn)$, 更改为$e^t_i = v^Ttanh(W_hh_i + W_ss_t + W_c c^t_i + b_{attn})$, 其中$c^t = \sum_{t’=0}^{t-1}a^{t’}$。 </p>
<p>同时更新了loss function。原先的loss function为 $loss_t=-logP(w_{target})$, $loss = \frac{1}{T} \sum^{T}_{t=0}loss_t$, 现在增加一项, $loss_t = -logP(w_{target}) + \lambda \sum_{i}min(a^t_i, c^t_i)$。</p>
<p>感想：</p>
<ul>
<li>这个去重复的想法还是比较简单的直接明了，但是效果挺好，从头看来就是生成一样的，就是关注了一样的，那我怎么关注不一样的，就设计减少关注已经关注的。</li>
</ul>
<p><br></p>
<h4 id="Incorporating-Copying-Mechanism-in-Sequence-to-Sequence-Learning"><a href="#Incorporating-Copying-Mechanism-in-Sequence-to-Sequence-Learning" class="headerlink" title="Incorporating Copying Mechanism in Sequence-to-Sequence Learning"></a>Incorporating Copying Mechanism in Sequence-to-Sequence Learning</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1603.06393.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06393.pdf</a></li>
<li>hku, huawei</li>
<li>ACL 2016</li>
</ul>
<p>看了一下，跟上一篇的思想整体是差不多的。【当然他比上一篇早一年orz】，上一篇用$p_{gen}$来作为权重加权给generate 和 copy加权和，这边是直接相加。</p>
<p>主要就是对从decoder 的hidden state到输出和更新$s_t$做了处理。</p>
<p>【这篇没有细看，有需求时候再重温】</p>
<p><br></p>
<h4 id="Deep-Communicating-Agents-for-Abstractive-Summarization"><a href="#Deep-Communicating-Agents-for-Abstractive-Summarization" class="headerlink" title="Deep Communicating Agents for Abstractive Summarization"></a>Deep Communicating Agents for Abstractive Summarization</h4><ul>
<li>文章地址：<a href="http://www.aclweb.org/anthology/N18-1150" target="_blank" rel="noopener">http://www.aclweb.org/anthology/N18-1150</a></li>
<li>MSRA, JD, UW</li>
<li>NAACL 2018</li>
</ul>
<p>这篇文章，作者主要针对的是长文本的ABS问题。作者把一篇长文本用多个agent去分别处理，每一段用一个agent来encoder，最后再用一个decoder对所有的中间向量进行解码生成结果。</p>
<p><img src="/2019/02/20/扫文笔记2/21.png" width="100%"></p>
<p><img src="/2019/02/20/扫文笔记2/22.png" width="40%"></p>
<p>每一个agent对应有两个encoder组成，一个是local encoder，一个是contextual encoder。local encoder就是将对应段落的每个词喂到BLSTM中去，然后生成对应的hidden state。</p>
<p><img src="/2019/02/20/扫文笔记2/23.png" width="50%"></p>
<p>然后对于contextural encoder，对于每一层contextural encoder，都会接收</p>
<p><img src="/2019/02/20/扫文笔记2/24.png" width="50%"></p>
<ul>
<li>上一层输出的hidden state</li>
<li>左侧连接的cell和右侧连接的cell的hidden state</li>
<li>来自其他agent的message vector【这个就是同层其他agent的hidden state取平均】</li>
</ul>
<p><img src="/2019/02/20/扫文笔记2/25.png" width="30%"></p>
<p><img src="/2019/02/20/扫文笔记2/26.png" width="50%"></p>
<p>对于decoder，他使用的是单层的LSTM，第一个agent的最后一层的hidden state是他的init state。每经过一个step，就会预测出一个新单词，并计算出一个新状态$s_t$。decoder上使用一个多层次的attention机制，首先是词的attention。</p>
<p><img src="/2019/02/20/扫文笔记2/27.png" width="50%"></p>
<p>然后通过$l_a^t$获得新的$c_a^t$。</p>
<p><img src="/2019/02/20/扫文笔记2/28.png" width="20%"></p>
<p>再用$c_a^t$计算全文的attention。</p>
<p><img src="/2019/02/20/扫文笔记2/29.png" width="40%"></p>
<p><img src="/2019/02/20/扫文笔记2/30.png" width="20%"></p>
<p>然后根据这个$c^*_t$用MLP+softmax计算词汇分配在词汇表上的概率</p>
<p>另外，作者也提出了用pointer network的思想，copy和generate结合。最后的损失函数是最大似然 + 语义衔接损失【用 $\lambda  加权重】+ 增强学习的损失函数。</p>
<p>感想：</p>
<ul>
<li>这种每段单独处理应该是对长文本信息保留的比较好的办法了，以及多层attention也非常的合适，有借鉴意义。</li>
<li>这个state $s_t$那边有些玄学，不知道生成的word用来干嘛，咋搞出$s_t$。</li>
</ul>
<p><br></p>
<h4 id="Fast-Abstractive-Summarization-with-Reinforce-Selected-Sentence-Rewriting"><a href="#Fast-Abstractive-Summarization-with-Reinforce-Selected-Sentence-Rewriting" class="headerlink" title="Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting"></a>Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</h4><ul>
<li>文章地址：<a href="http://www.aclweb.org/anthology/P18-1063" target="_blank" rel="noopener">http://www.aclweb.org/anthology/P18-1063</a></li>
<li>UNC Chapel Hill</li>
<li>ACL 2018</li>
</ul>
<p>整个文章的思路是先从文本中抽取重要的句子，然后将这每句话生成对应的摘要，连接起来。也就是分为extractor和abstract两个部分。</p>
<p><img src="/2019/02/20/扫文笔记2/31.png" width="90%"></p>
<p>作者先用一个分层的神经网络（Temporal Convolutional Networks）来学习文本中的sentence representation，并且为了学习整个文章的意思以及较长距离句子之间的相关性，在TCN的输出中作者又加上了一个双层的LSTM，获得最终对每个句子的representation $h_j$。</p>
<p>随后我们又用一个RNN来训练一个pointer-network来训练sentence-selection。整个pointer的过程是一个两跳的attention，$z_t$是decoder部分LSTM后的输出，h先和z做attention计算，来获取context vector $e_t$，然后再用这个context vector和h在做一次attention计算，来选择挑选哪一个句子。</p>
<p><img src="/2019/02/20/扫文笔记2/32.png" width="40%"></p>
<p><img src="/2019/02/20/扫文笔记2/33.png" width="50%"></p>
<p>abstract的部分用的就是标准的encoder-decoder模型，并且加上copynet来处理OOV单词。</p>
<p>模型的训练是将两个part分开训练的。extractor部分loss function使用的是和最相近句子的cross-entropy, abstractor部分是语言模型的cross-entropy，先各自训练，然后再用强化学习进行训练，强化学习只调整extractor的参数，用rouge作为reward。另外我们还对repeat做了处理，在beam search时候，保留所有的选项，按照trim重复排序，然后选择。</p>
<p>并且由于每个选出来的句子解码不想影响，所以可以并行，加速计算。</p>
<p><br></p>
<h4 id="Paragraph-level-Neural-Question-Generation-with-Maxout-Pointer-and-Gated-Self-attention-Networks"><a href="#Paragraph-level-Neural-Question-Generation-with-Maxout-Pointer-and-Gated-Self-attention-Networks" class="headerlink" title="Paragraph-level Neural Question Generation  with Maxout Pointer and Gated Self-attention Networks"></a>Paragraph-level Neural Question Generation  with Maxout Pointer and Gated Self-attention Networks</h4><ul>
<li>EMNLP 2018</li>
</ul>
<p>文章是用来解决Question Generation的，这边有启发的一个点是，他对repeat进行了处理，作者猜测seq2seq模型生成repeat的原因是因为本身原有的input的文本中就有很多重复的词语。并且在原有的copy操作中，我们是如下图操作。也就是对当前词在input地方出现的attention值全加，这样由于部分词在input中出现的次数多，所以很多情况下加和值也比较大，因而在generate和copy中就会出现的多。因而导致了repeat的问题。于是作者提出了改进方式，为下一张图，改成每次取input中该词的max值，来避免叠加使增大。</p>
<p><img src="/2019/02/20/扫文笔记2/34.png" width="50%"></p>
<p><img src="/2019/02/20/扫文笔记2/35.png" width="50%"></p>
<p><br></p>
<h4 id="Generative-Adversarial-Network-for-Abstractive-Text-Summarization"><a href="#Generative-Adversarial-Network-for-Abstractive-Text-Summarization" class="headerlink" title="Generative Adversarial Network for Abstractive Text Summarization"></a>Generative Adversarial Network for Abstractive Text Summarization</h4><ul>
<li>文章地址：<a href="https://likicode.com/upload/AAAI_accepted_paper.pdf" target="_blank" rel="noopener">https://likicode.com/upload/AAAI_accepted_paper.pdf</a> </li>
<li>补充材料：<a href="https://likicode.com/textsum/" target="_blank" rel="noopener">https://likicode.com/textsum/</a></li>
<li>Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences</li>
<li>AAAI 2018 短文</li>
</ul>
<p>文章提出用生成对抗网络来进行summarization。用Generator来生成Summary，用Discriminative来区分Summary是生成还是人写的【也就是一个简单的二分类问题】,分别单独pre-train以后进行训练网路。</p>
<p>文章的generator是学习了pointer-network的，discriminative就是一个CNN。</p>
<p>discriminative的loss函数就是最小化分类错误，generator的loss function是组合了maximum-likelihood loss 以及强化学习的loss（$J_{pg}$)。其中$R_D^{G_\Theta}$是generator出来的summary扔到discriminative中被认为是人写的summaryd概率减去了一个baseline value（用来keep unbiased)</p>
<p><img src="/2019/02/20/扫文笔记2/36.png" width="50%"></p>
<p><img src="/2019/02/20/扫文笔记2/37.png" width="50%"></p>
<p><br></p>
<h4 id="Pretraining-Based-Natural-Language-Generation-for-Text-Summarization"><a href="#Pretraining-Based-Natural-Language-Generation-for-Text-Summarization" class="headerlink" title="Pretraining-Based Natural Language Generation for Text Summarization"></a>Pretraining-Based Natural Language Generation for Text Summarization</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1902.09243.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1902.09243.pdf</a></li>
<li>Microsoft Research Asia</li>
<li>arXiv</li>
</ul>
<p>文章用最热的BERT来做Text Summarization。考虑到BERT是用双向语言模型训练的，因此在普通decode的时候，只有前面的输出，不了解后文的信息，会影响输出的结果。因此作者提出用两个阶段的decoder，第一个stage就是decoder正常的输出，输出的为我们summarization的草稿结果，然后用这个结果作为第2阶段的输入，在这个阶段我们每次mask一个词语，然后来预测这个词语的正确输出。两个stage的decoder用的都是N层的transformer，参数共享。训练同样用的是强化学习，用rouge来衡量reward值，多方计算loss值。</p>
<p><img src="/2019/02/20/扫文笔记2/38.png" width="80%"></p>
<p>感想：</p>
<ul>
<li>的确，生成的结果有所提升，但感觉和BERT在这篇文章上带来的提升没有其他那么明显，应该是还有哪里没有处理好感觉。</li>
</ul>
<p><br></p>
<h4 id="Faithful-to-the-Original-Fact-Aware-Neural-Abstractive-Summarization"><a href="#Faithful-to-the-Original-Fact-Aware-Neural-Abstractive-Summarization" class="headerlink" title="Faithful to the Original: Fact-Aware Neural Abstractive Summarization"></a>Faithful to the Original: Fact-Aware Neural Abstractive Summarization</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1711.04434.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1711.04434.pdf</a></li>
<li>The Hong Kong Polytechnic University</li>
<li>ACL 2018</li>
</ul>
<p>文章提出原有的模型生成的summary很多都偏离了事实本身，也就是说其实对原文事实理解不够，因此文章提出首先利用OpenIE抽出句子的三元组，然后使用这样的三元组和原有的文章一起生成summary。在整个模型里面和传统的基本相似，三元组和原生的word embedding分别经过BiGRU进行encode，然后分别进行attention计算，生成两个c，两者拼接输入MLP后获得权重，用于两个c的加权和，生成最后的c。然后在用于进行常规的decode。</p>
<p><img src="/2019/02/20/扫文笔记2/39.png" width="80%"></p>
<p>感想：</p>
<ul>
<li>文章主要点在用了三元组信息，别的信息很常规。</li>
</ul>
<p><br></p>
<h4 id="Sliced-Recurrent-Neural-Networks"><a href="#Sliced-Recurrent-Neural-Networks" class="headerlink" title="Sliced Recurrent Neural Networks"></a>Sliced Recurrent Neural Networks</h4><ul>
<li>文章地址：<a href="http://aclweb.org/anthology/C18-1250" target="_blank" rel="noopener">http://aclweb.org/anthology/C18-1250</a></li>
<li>SJTU 刘功申老师组</li>
<li>CLOING 2018</li>
</ul>
<p>文章提出一种SRNN（切片循环RNN），来提高RNN的速度。思路感觉蛮简单清晰的，就是将长句子切短，一层层切到最短，然后最短的part再过GRU，不断向上传递，就是一个树状数组感觉。并且论文还证明了再激励函数是linear的时候，并且符合一定的条件，RNN就是SRNN。</p>
<p><img src="/2019/02/20/扫文笔记2/40.png" width="80%"></p>
<p> <br></p>
<h4 id="Abstractive-Document-Summarization-with-a-Graph-Based-Attentional-Neural-Model"><a href="#Abstractive-Document-Summarization-with-a-Graph-Based-Attentional-Neural-Model" class="headerlink" title="Abstractive Document Summarization with a Graph-Based Attentional Neural Model"></a>Abstractive Document Summarization with a Graph-Based Attentional Neural Model</h4><ul>
<li>文章地址：<a href="http://www.aclweb.org/anthology/P17-1108" target="_blank" rel="noopener">http://www.aclweb.org/anthology/P17-1108</a></li>
<li>Peking University</li>
<li>ACL 2017 （outstanding paper）</li>
</ul>
<p>文章利用传统的encoder-decoder的模型进行改造，分word layer和sentence layer。在encoder部分将每个sentence的每个word进行encoder获得sentence的vector。然后decoder部分也是先decoder出sentence的vector。这边的vector可以选用传统的attention方式，对encoder和decoder方式的sentence vector进行attention计算。</p>
<p>同时他也提出了新的attention思想。新的想法参考的是page rank的想法，认为如果这个句子被link的多就重要</p>
<p><img src="/2019/02/20/扫文笔记2/41.png" width="50%"></p>
<p>我们令n为encoder和decoder句子总个数，其中W矩阵为n*n，$W(i,j) = h_i^TMh_j$,M矩阵是一个parameter matrix，D是一个对角矩阵，D(i,i)等于W中第i列的和（应该就是一个归一化操作）。y为一个每个元素都为1/n的n维矩阵。 f就是计算了每轮迭代中每个句子的分数。上文的公式，可以被计算出来为。【这边的一个思想考虑到了decoder的句子不仅和encoder有关，还和decoder已经生成的句子相关】</p>
<p><img src="/2019/02/20/扫文笔记2/42.png" width="30%"></p>
<p>整个过程的方式是</p>
<p><img src="/2019/02/20/扫文笔记2/43.png" width="90%"></p>
<p>对于在decoding部分的想法，对于OOV问题，他这边使用的是将命名实体替换为@entity#的形式，在解码的时候再替换回来。然后分级解析句子和词语。感觉句子级别的beam search，是对生成的N个句子选N个不同的origin sentence（选取attention最大的）作为ref，且这些sentence原先没有用作过ref。这些选出的句子给单词级别的beam search做ref，单词级别的beam search在传统的bem search基础上增加了一个附加项</p>
<p><img src="/2019/02/20/扫文笔记2/44.png" width="70%"></p>
<p>ref是用来度量两个文本序列中所含相同bigram比例的函数。</p>
<p>感想：</p>
<ul>
<li><p>page rank的思想我觉得很work</p>
<p>​</p>
</li>
</ul>
<p>以及另外两篇work【没有细读，扫了一下】</p>
<ul>
<li>improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling【<a href="http://aclweb.org/anthology/D18-1205】" target="_blank" rel="noopener">http://aclweb.org/anthology/D18-1205】</a></li>
<li>Improving Neural Abstractive Document Summarization with Structural Regularization【<a href="http://aclweb.org/anthology/D18-1441】" target="_blank" rel="noopener">http://aclweb.org/anthology/D18-1441】</a></li>
<li>两篇是同班人，投在EMNLP2017上的</li>
<li>这篇文章和其他文章都提到了seq2seq在双层模型的时候可能生成相似的句子，如何去解决这些问题的方法。</li>
<li>另一篇文章提出考虑summary覆盖率，也就是说我一个好的summarization应该对全文有较高的覆盖率。</li>
<li>双层的attention问题，一篇的做法是，算是自己管自己算，在最后生成c的时候会把sentence的attn和word的attn乘起来再乘以word。</li>
</ul>
<p><br></p>
<h4 id="Retrieve-Rerank-and-Rewrite-Soft-Template-Based-Neural-Summarization"><a href="#Retrieve-Rerank-and-Rewrite-Soft-Template-Based-Neural-Summarization" class="headerlink" title="Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization"></a>Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization</h4><ul>
<li>文章地址：<a href="https://aclweb.org/anthology/P18-1015" target="_blank" rel="noopener">https://aclweb.org/anthology/P18-1015</a></li>
<li>Hong Kong Polytechnic University</li>
<li>ACL 2018</li>
</ul>
<p>首先第一感觉就是文章写的非常清晰易懂，逻辑清晰也非常好理解。</p>
<p>文章提出使用一个template来辅助生成更好的summarization。文章的过程一共分为三个步骤，</p>
<ul>
<li><p>retrieve — 根据Information Retrieve平台，根据input sentence，找出top30的search result 作为candidate templates</p>
</li>
<li><p>rerank — 将input sentence和每一个candidate templates输入到双层的RNN中，获得最后一个hidden state的表示（用的是同一个encoder）。rerank的思想是用原文和templates的rouge值来比较candidate。这边的rouge值是通过这个式子来计算的，其中$h_x$是input通过BiRNN的last hidden state的表示，$h_r$是template的对应输出。这样通过这个计算，找到分值最高的template作为最终的template。</p>
<p><img src="/2019/02/20/扫文笔记2/45.png" width="50%"></p>
</li>
<li><p>rewrite — 将template的hidden所有输出和input的hidden所有输出拼在一起作为encoder输出的hidden states。通过以下式子，计算出output。这边的decoder用的就是最为传统的attention-RNN-Decoder。</p>
<p><img src="/2019/02/20/扫文笔记2/46.png" width="40%"></p>
<p><img src="/2019/02/20/扫文笔记2/47.png" width="30%"></p>
</li>
<li><p>loss function — 这边的loss function由两部分组成，一部分就是常规的NLL，另一部分是上面$s(r,x)$和rouge真实值的cross entropy。两个部分直接相加，就是最终的loss function。</p>
<p><img src="/2019/02/20/扫文笔记2/48.png" width="50%"></p>
</li>
<li><p>其他evaluate方法 — 文章主要用rouge来作为evalute，还选用了其他的评判标准辅助判断：</p>
<ul>
<li>LEN_DIF 生成的句子和groud truth的长度差异</li>
<li>LESS_3 少于三个token的summary【可以直接认为没有可读性【dataset为gigaword】</li>
<li>COPY 除去stopwords外从原文copy的单词比例【比例越高说明越像copy而不是summarization】</li>
<li>NEW_NE 没有在原文或者groud_truth中出现但是出现在summary中的named entities【越大表示越没有可信度】</li>
<li>评测结果如下图</li>
</ul>
</li>
</ul>
<p><img src="/2019/02/20/扫文笔记2/49.png" width="50%"></p>
<p>网上解读的截图总结如下，整篇文章还是很有想法的，虽然我觉得那个hidden state可以这样拼也是很神奇。evalutate的copy 和new_ne有点意思啊。</p>
<p><img src="/2019/02/20/扫文笔记2/50.png" width="80%"></p>
<p><br></p>
<h4 id="A-Hierarchy-to-Sequence-Attentional-Neural-Machine-Translation-Model"><a href="#A-Hierarchy-to-Sequence-Attentional-Neural-Machine-Translation-Model" class="headerlink" title="A Hierarchy-to-Sequence Attentional Neural Machine Translation Model"></a>A Hierarchy-to-Sequence Attentional Neural Machine Translation Model</h4><ul>
<li>IEEE/ACM Transactions on Audio, Speech, and Language Processing【B类期刊】，2018</li>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8246560" target="_blank" rel="noopener">https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8246560</a></li>
<li>Chinese Academy of Sciences</li>
<li>没有仔细读，看了个大概的模型</li>
</ul>
<p>文章提出hierarchy的NMT模型，word level + sentence level + 两层的attention【一个只考虑这个正在翻译的句子的word hidden states，另一个考虑所有句子的hidden states】</p>
<p>没有提到速度问题，我觉得应该是那个inter attention要求并行</p>
<p>粗看文章感觉也没有什么新奇的点？不知道为啥能效果好</p>
<p><img src="/2019/02/20/扫文笔记2/51.png" width="100%"></p>
<p><br></p>
<h4 id="A-Hierarchical-Neural-Network-for-Sequence-to-Sequences-Learning"><a href="#A-Hierarchical-Neural-Network-for-Sequence-to-Sequences-Learning" class="headerlink" title="A Hierarchical Neural Network for Sequence-to-Sequences Learning"></a>A Hierarchical Neural Network for Sequence-to-Sequences Learning</h4><ul>
<li><a href="https://arxiv.org/pdf/1811.09575.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.09575.pdf</a></li>
<li>arxiv</li>
</ul>
<p>文章提出的也是一个hierarchical的seq2seq模型，微微不同的是最后有个微调节part。这篇文章用的是4层LSTM233。</p>
<p>感觉没什么新点，感觉最后的fine部分有越来越多的趋势在里面</p>
<p>要写hierarchical时候再来找找有没有有用的part</p>
<p><img src="/2019/02/20/扫文笔记2/52.png" width="100%"></p>
<p><br></p>
<p>这堆hierarchical模型的论文中再加上一篇类似得文章</p>
<ul>
<li>A Hierarchical Attention Seq2seq Model with CopyNet for Text Summarization</li>
<li><a href="https://ieeexplore.ieee.org/document/8410295" target="_blank" rel="noopener">https://ieeexplore.ieee.org/document/8410295</a></li>
<li>2018 International Conference on Robots &amp; Intelligent System【不在ccf中，不知道是啥】</li>
<li>很迷，感觉分块的大致思想是差不多的，但他蜜汁速度还是慢的？？？文章写的太乱了，没怎么细看。感觉就是很迷？就是加了一大堆模块，把能用的都扔上去的感觉，不知道在干嘛的感觉。</li>
</ul>
<p><br></p>
<h4 id="A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents"><a href="#A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents" class="headerlink" title="A Hierarchical Neural Autoencoder for Paragraphs and Documents"></a>A Hierarchical Neural Autoencoder for Paragraphs and Documents</h4><ul>
<li>Standford NLP</li>
<li>ACL 2015</li>
<li><a href="https://arxiv.org/pdf/1506.01057.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1506.01057.pdf</a></li>
</ul>
<p>文章提出了三种方法进行autoencoder文章，分别为standard，hierarchical和attention hierarchical。现在看下来最大的感受就是大家都考虑过这个模型，但从没有人断掉过decoder中两个句子的连接？就很奇怪2333</p>
<p>用的是最基础的LSTM，代码是matlab的。。[<a href="https://github.com/jiweil/Hierarchical-Neural-Autoencoder]" target="_blank" rel="noopener">https://github.com/jiweil/Hierarchical-Neural-Autoencoder]</a></p>
<p><img src="/2019/02/20/扫文笔记2/53.png" width="100%"></p>
<p><br></p>
<p>hierical结构中还有两篇相关的论文，<a href="https://zhuanlan.zhihu.com/p/31170263在这个blog中提到了，是两个对话系统相关的。第二个还用了一个latent" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31170263在这个blog中提到了，是两个对话系统相关的。第二个还用了一个latent</a> variable产生了多样性。可以有所借鉴。还没细看。</p>
<p><br></p>
<h4 id="Reducing-BERT-Pre-Training-Time-from-3-Days-to-76-Minutes"><a href="#Reducing-BERT-Pre-Training-Time-from-3-Days-to-76-Minutes" class="headerlink" title="Reducing BERT Pre-Training Time from 3 Days to 76 Minutes"></a>Reducing BERT Pre-Training Time from 3 Days to 76 Minutes</h4><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1904.00962.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.00962.pdf</a></li>
<li>google brain</li>
<li>arxiv 2019</li>
</ul>
<p>一般加速deep learning使用的方法都是用多个卡来并行处理optimization的process。因为不同layers之间数据存在依赖性，所以不同层的parallelize并不可行。因此一般的并行主要关注的是maximizing batch size来减少运行时间。但是直接增加batch size会存在一个generaltization gap。整体而言就是说我增大batch会降低我的acc，用多台机器一起算，还会让我调整很久的hyper-parameters，很麻烦。所以要做一个增加batch，不影响acc，还减少了训练时间。<br>前人也提出了很多技巧，比如说</p>
<ul>
<li>扩大batch size k倍时，learning rate乘以√k ，更好的是LR也是乘以k来保持其他超参数的不变。</li>
<li>另外还会存在learning rate很大，这样开始训练时候就会难以稳定，因此提出了一个learning rate warmup的想法，也就是开始时候用一个smaller的lr，然后慢慢增大到较大的值。</li>
<li>large batch的training容易达到一个较高的training accuracy，但是很难到达一个很高的text accuarcy</li>
</ul>
<p>文章提出了LAMB-layer-wise Adaptive Moments optimizer for Batch training. 伪代码如下：</p>
<p><img src="/2019/02/20/扫文笔记2/54.png" width="80%"></p>
<p>文章阐述了四点这个算法和以前相关优化的不同，but我对optimizer一脸懵逼，没有特别看懂2333.</p>
<p>算法的结果是，可以参考一下速度对比一般要列出那些数据。</p>
<p><img src="/2019/02/20/扫文笔记2/55.png" width="60%"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/15/NLP相关/" rel="next" title="NLP各种相关模块">
                <i class="fa fa-chevron-left"></i> NLP各种相关模块
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/27/speed-survey/" rel="prev" title="abstractive summarization 模型速度问题">
                abstractive summarization 模型速度问题 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
     <div id="gitalk-container"></div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yunyan.hong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Neural-Attention-Model-for-Abstractive-Sentence-Summariztion"><span class="nav-number">1.</span> <span class="nav-text">A Neural Attention Model for Abstractive Sentence Summariztion</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Abstractive-Sentence-Summarization-with-Attentive-Recurrent-Neural-Networks"><span class="nav-number">2.</span> <span class="nav-text">Abstractive Sentence Summarization with Attentive Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Abstractive-Text-Summarization-Using-Sequence-to-Sequence-RNNs-and-Beyond"><span class="nav-number">3.</span> <span class="nav-text">Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Selective-Encoding-for-Abstractive-Sentence-Summarization"><span class="nav-number">4.</span> <span class="nav-text">Selective Encoding for Abstractive Sentence Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Global-Encoding-for-Abstractive-Summarization"><span class="nav-number">5.</span> <span class="nav-text">Global Encoding for Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Controllable-Abstractive-Summarization"><span class="nav-number">6.</span> <span class="nav-text">Controllable Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Deep-Reinforced-Model-For-Abstractive-Summarization"><span class="nav-number">7.</span> <span class="nav-text">A Deep Reinforced Model For Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Controlling-Output-Length-in-Neural-Encoder-Decoders"><span class="nav-number">8.</span> <span class="nav-text">Controlling Output Length in Neural Encoder-Decoders</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-contextualized-word-representations"><span class="nav-number">9.</span> <span class="nav-text">Deep contextualized word representations</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Attention-Is-All-You-Need"><span class="nav-number">10.</span> <span class="nav-text">Attention Is All You Need</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Reinforced-Topic-Aware-Convolutional-Sequence-to-Sequence-Model-for-Abstractive-Text-Summarization"><span class="nav-number">11.</span> <span class="nav-text">A Reinforced Topic-Aware Convolutional Sequence-to-Sequence Model for Abstractive Text Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><span class="nav-number">12.</span> <span class="nav-text">Get To The Point: Summarization with Pointer-Generator Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Incorporating-Copying-Mechanism-in-Sequence-to-Sequence-Learning"><span class="nav-number">13.</span> <span class="nav-text">Incorporating Copying Mechanism in Sequence-to-Sequence Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Communicating-Agents-for-Abstractive-Summarization"><span class="nav-number">14.</span> <span class="nav-text">Deep Communicating Agents for Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Fast-Abstractive-Summarization-with-Reinforce-Selected-Sentence-Rewriting"><span class="nav-number">15.</span> <span class="nav-text">Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Paragraph-level-Neural-Question-Generation-with-Maxout-Pointer-and-Gated-Self-attention-Networks"><span class="nav-number">16.</span> <span class="nav-text">Paragraph-level Neural Question Generation  with Maxout Pointer and Gated Self-attention Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Generative-Adversarial-Network-for-Abstractive-Text-Summarization"><span class="nav-number">17.</span> <span class="nav-text">Generative Adversarial Network for Abstractive Text Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pretraining-Based-Natural-Language-Generation-for-Text-Summarization"><span class="nav-number">18.</span> <span class="nav-text">Pretraining-Based Natural Language Generation for Text Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Faithful-to-the-Original-Fact-Aware-Neural-Abstractive-Summarization"><span class="nav-number">19.</span> <span class="nav-text">Faithful to the Original: Fact-Aware Neural Abstractive Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sliced-Recurrent-Neural-Networks"><span class="nav-number">20.</span> <span class="nav-text">Sliced Recurrent Neural Networks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Abstractive-Document-Summarization-with-a-Graph-Based-Attentional-Neural-Model"><span class="nav-number">21.</span> <span class="nav-text">Abstractive Document Summarization with a Graph-Based Attentional Neural Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Retrieve-Rerank-and-Rewrite-Soft-Template-Based-Neural-Summarization"><span class="nav-number">22.</span> <span class="nav-text">Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Hierarchy-to-Sequence-Attentional-Neural-Machine-Translation-Model"><span class="nav-number">23.</span> <span class="nav-text">A Hierarchy-to-Sequence Attentional Neural Machine Translation Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Hierarchical-Neural-Network-for-Sequence-to-Sequences-Learning"><span class="nav-number">24.</span> <span class="nav-text">A Hierarchical Neural Network for Sequence-to-Sequences Learning</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#A-Hierarchical-Neural-Autoencoder-for-Paragraphs-and-Documents"><span class="nav-number">25.</span> <span class="nav-text">A Hierarchical Neural Autoencoder for Paragraphs and Documents</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Reducing-BERT-Pre-Training-Time-from-3-Days-to-76-Minutes"><span class="nav-number">26.</span> <span class="nav-text">Reducing BERT Pre-Training Time from 3 Days to 76 Minutes</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yunyan.hong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '214b2aee446518ccee23',
          clientSecret: '6b5e9007abf73fbcdce7d46cc4ae66a5fa5e6a3d',
          repo: 'hongyunyan.github.io',
          owner: 'hongyunyan',
          admin: ['hongyunyan'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
