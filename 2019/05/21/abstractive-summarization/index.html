<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="A Robust Abstractive System for Cross-Lingual Summarization 文章地址：http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf Columbia University NAACL 2019  文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种">
<meta property="og:type" content="article">
<meta property="og:title" content="新一轮Summarization以及相关扫文工作">
<meta property="og:url" content="http://yoursite.com/2019/05/21/abstractive-summarization/index.html">
<meta property="og:site_name" content="Yunyan Hong">
<meta property="og:description" content="A Robust Abstractive System for Cross-Lingual Summarization 文章地址：http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf Columbia University NAACL 2019  文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/1.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/2.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/3.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/4.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/5.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/6.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/7.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/8.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/9.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/10.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/11.png">
<meta property="og:updated_time" content="2019-05-29T06:54:19.878Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="新一轮Summarization以及相关扫文工作">
<meta name="twitter:description" content="A Robust Abstractive System for Cross-Lingual Summarization 文章地址：http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf Columbia University NAACL 2019  文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种">
<meta name="twitter:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/05/21/abstractive-summarization/">





  <title>新一轮Summarization以及相关扫文工作 | Yunyan Hong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yunyan Hong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/21/abstractive-summarization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yunyan.hong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yunyan Hong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">新一轮Summarization以及相关扫文工作</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-21T10:48:00+08:00">
                2019-05-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="A-Robust-Abstractive-System-for-Cross-Lingual-Summarization"><a href="#A-Robust-Abstractive-System-for-Cross-Lingual-Summarization" class="headerlink" title="A Robust Abstractive System for Cross-Lingual Summarization"></a>A Robust Abstractive System for Cross-Lingual Summarization</h3><ul>
<li>文章地址：<a href="http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf</a></li>
<li>Columbia University</li>
<li>NAACL 2019</li>
</ul>
<p>文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种摘要的任务中，一般的选择有两种，一种是先翻译再摘要，另一种是先摘要再翻译。以前的研究中，因为先摘要再翻译可以减少计算量，所以选择的是先摘要后翻译。但是因为以前主要研究的是都是大语种，但是小语种容易出现没有足够的小语种训练集来训练一个优秀的摘要模型，因此作者在文中提出先翻译再摘要的做法。</p>
<p>作者的实验方式，是选用了The New York Times的数据集，将他们先翻译成三种小语种语言，再将翻译后的文本再翻译为英文。然后用这个翻译回来的noisy的英文文本去训练abstractor模型，同时还利用三种语言翻译回来的文本混合，训练一个mixed的模型。然后进行测试。测试结果这样的摘要结果优于直接用Pointer Generator的模型去对noisy文本进行摘要。【讲道理这不是很显然吗，一个都不符合你的training data的类型？】同时他还选用了真实的小语种语料来测试模型，并用人工判读的方式来测评摘要的内容和流畅度。另外作者还找了一个另一个小语言DUC数据，去测试不同的模型，来证明自己训练的模型的可行性。</p>
<p>文章主要做的事情可以理解为用抽象模型删除难翻译的短语并生成新文本的能力，该模型学会从不流畅的输入中产生流畅的摘要。</p>
<p>个人感受：这应该算一个蛮新的课题，不过他的模型上并没有什么新的内容，就只是用的是不流畅的文本。 </p>
<p><br></p>
<h3 id="Abstractive-Summarization-of-Reddit-Posts-with-Multi-level-Memory-Networks"><a href="#Abstractive-Summarization-of-Reddit-Posts-with-Multi-level-Memory-Networks" class="headerlink" title="Abstractive Summarization of Reddit Posts with Multi-level Memory Networks"></a>Abstractive Summarization of Reddit Posts with Multi-level Memory Networks</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1811.00783.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.00783.pdf</a></li>
<li>Seoul National University</li>
<li>NAACL 2019</li>
</ul>
<p>文章主要做了两件事情，一件事情是提出了一个新的数据集，Reddit TITU dataset，是从一个论坛上得到的数据集，每个文本对应一个长摘要和一个短摘要，并且数据集存在一个优势：新闻类的数据的重要信息普遍在开头的句子中，因此数据集中包含了一定的结构信息。但是这个数据集的重点信息没有明显的结构分布，所以更为普适。</p>
<p>另一件事情，是提出了一个新的摘要模型，Multi-Level Memory Network，感觉没有非常看懂这个神奇的模型，感觉最底层是一个多层的CNN，CNN层通过多个不同跳跃格数的卷积，生成对应的信息，写入memory cells。memory cells有两层，一层叫input，一层叫output。然后再用一个decoder来生成单词。【主要是memory networks没有看懂，以后跟着代码看吧】</p>
<p><img src="/2019/05/21/abstractive-summarization/1.png" width="100%" height="80%"></p>
<p><br></p>
<h3 id="SEQ3-Differentiable-Sequence-to-Sequence-to-Sequence-Autoencoder-for-Unsupervised-Abstractive-Sentence-Compression"><a href="#SEQ3-Differentiable-Sequence-to-Sequence-to-Sequence-Autoencoder-for-Unsupervised-Abstractive-Sentence-Compression" class="headerlink" title="SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression"></a>SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1904.03651.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.03651.pdf</a></li>
<li>National Technical University of Athens</li>
<li>NAACL 2019</li>
</ul>
<p>文章做的是一个无监督的文本摘要（也就是文本压缩），利用的是一个类似Autoencoder的结构，前面一部分是对文本的压缩，后一部分是根据摘要结果对文本进行重建。然后计算的方式都是Seq2Seq的结构。计算的loss一个是输入文本和重建文本的diff，一个是topic的diff（原文本和压缩后的文本，用TF-IDF计算),摘要和language model的KL分布diff，长度diff四个部分。</p>
<p><img src="/2019/05/21/abstractive-summarization/2.png" width="60%" height="80%"></p>
<p><br></p>
<h3 id="BI-DIRECTIONAL-ATTENTION-FLOW-FOR-MACHINE-COMPREHENSION"><a href="#BI-DIRECTIONAL-ATTENTION-FLOW-FOR-MACHINE-COMPREHENSION" class="headerlink" title="BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION"></a>BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.01603.pdf</a></li>
</ul>
<ul>
<li>Allen Institute for Artificial Intelligence,Washington</li>
<li>ICLR，2017</li>
</ul>
<p>文章主要的任务是通过阅读理解来回答question。也就是说给定一个文本，再给一个问题，让系统学习回答正确的答案。</p>
<p>模型一共分为6层，如图所示：</p>
<p><img src="/2019/05/21/abstractive-summarization/3.png" width="100%" height="80%"></p>
<p>最下面三层主要的工作就是一个embedding的工作，一层是character embedding层，用字符级的CNN，CNN在整个宽度上max-pooling。另一层是一个word embedding层，用Glove预训练的词向量。然后这两个输入拼接起来输入Highway Network得到对应的context vector X(d,T)和query vector Q(d,J)。然后根据得到的vector通过bi-LSTM，获得对应的输出，H(2d, T), U(2d, J)。通过这样的编码操作，我们已经可以得到context和query整合了不同层次的表征信息。</p>
<p>下一层是Attention Flow Layer，输入是上层得到的H和U，输出是具有问题感知的上下文表征G。在这层中我们是要计算两个方向的attention : context to query 以及query to context。首先我们要先计算一个这两个attention都要用到的一个相似矩阵。$S_{tj} = \alpha (H_{:t}, U_{:j})$ 。其中$S_{tj}$表明的是第t个上下文中的单词和第j个query中的单词的相似度，其中$\alpha(h,u) = W_S^T[h;u;h \circ u]$。</p>
<p>Context-to-query Attention(C2Q)计算的是对每一个 context word 而言哪些 query words 和它最相关，所以$a_t = softmax(S_{t:})$, $\tilde{U_{:t}} = \sum_j a_{tj}U_{:j}$, 所以$\tilde{U}$是一个2d*T的矩阵，一个全新的问题编码。</p>
<p>Query-to-context Attention(Q2C)计算的是对每一个 query word 而言哪些 context words 和它最相关,因此这些单词就和答案有很大的关系。所以直接取了S矩阵中最大的那列（也就是那个单词对我们query的相似性最大）$b=softmax(max_{col}(S))$, $\tilde{h} = \sum_t b_t H_{:t}$,然后重复T次得到$\tilde{H}$,也是2d*T。</p>
<p>随后我们将这两个新编码经过一个MLP函数\beta的计算得到G， $G_{:t} = \beta(H_{:t}, \tilde{U_{:t}}, \tilde{H_{:t}})$, $\beta(h, \tilde{u}, \tilde{h}) = [h;\tilde{u}; h \circ \tilde{u}; h \circ \tilde{h}]$。（8d*T） </p>
<p>G通过一个BiLSTM得到M（2d*T），来捕获矩阵在时序上的依赖关系。最后的output层就是输出起始位置和结束为止的概率分布。$p^1 = softmax(W^T_{p1}[G;M])$, $p^2 = softmax(W^T_{p2}[G;M])$，损失函数是$L(\Theta ) = -1/N \sum_i^N[log(p1_{y1}) + log(p2_{y2})]$。</p>
<p>整体而言，文章的亮点是那个双向的attention思想，btw也是我第一次见到word embedding和char embedding组合的方式。</p>
<p><br></p>
<h3 id="Achieving-Open-Vocabulary-Neural-Machine-Translation-with-Hybrid-Word-Character-Models"><a href="#Achieving-Open-Vocabulary-Neural-Machine-Translation-with-Hybrid-Word-Character-Models" class="headerlink" title="Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"></a>Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1604.00788.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1604.00788.pdf</a></li>
<li>Stanford， Christopher D. Manning</li>
<li>ACL 2016</li>
</ul>
<p>文章的主要观点就是提出了下面这个模型，也就是说在输入或输出word层为unk的地方，加入一个char层进行embedding和decoder。这样来解决char层又慢又容易出现错误单词，word层有unk出现的问题。</p>
<p><img src="/2019/05/21/abstractive-summarization/4.png" width="50%" height="80%"></p>
<p>对于在char层的输入的states，作者提出，因为原先word层用于softmax的h‘，因为都是生成了unk，有很大概率比较相近，很容易产生相同的char，所以作者提出利用context c和hidden states单独训练用于char层输入的h’‘。整体格式和训练h’相同，只是是不同的W而已。</p>
<p><br></p>
<h3 id="Fully-Character-Level-Neural-Machine-Translation-without-Explicit-Segmentation"><a href="#Fully-Character-Level-Neural-Machine-Translation-without-Explicit-Segmentation" class="headerlink" title="Fully Character-Level Neural Machine Translation without Explicit Segmentation"></a>Fully Character-Level Neural Machine Translation without Explicit Segmentation</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1610.03017.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1610.03017.pdf</a></li>
<li>ETH Zu ̈rich</li>
<li>2017 【 这篇看的比较简略，和A Character-Level Decoder without Explicit Segmentation<br>for Neural Machine Translation是一波人写的，想法基本一致的】</li>
</ul>
<p>文章利用完全的char层进行NMT工作，这个图就是对输入信息的encoder方式，用CNN+highway network等进行encoder。decoder用的是<a href="https://arxiv.org/pdf/1603.06147.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06147.pdf</a>这篇文章中的。</p>
<p><img src="/2019/05/21/abstractive-summarization/5.png" width="100%" height="80%"></p>
<p><br></p>
<h3 id="HL-EncDec-A-Hybrid-Level-Encoder-Decoder-for-Neural-Response-Generation"><a href="#HL-EncDec-A-Hybrid-Level-Encoder-Decoder-for-Neural-Response-Generation" class="headerlink" title="HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation"></a>HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation</h3><ul>
<li>文章地址：<a href="https://www.aclweb.org/anthology/C18-1072" target="_blank" rel="noopener">https://www.aclweb.org/anthology/C18-1072</a></li>
<li>北京大学</li>
<li>ACL 2018</li>
</ul>
<p>用word层和char层一起生成语句回复。做的是中文的数据。用下图结构生成word 的embedding。用charCNN生成的embedding和word 本身的embedding相加，作为encoder结果。decoder就是要么生成word，要么就生成char，来避免oov。</p>
<p><img src="/2019/05/21/abstractive-summarization/6.png" width="80%" height="80%"></p>
<p><br></p>
<h3 id="HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization"><a href="#HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization" class="headerlink" title="HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"></a>HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1905.06566.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.06566.pdf</a></li>
<li>MSRA，Beijing</li>
<li>ACL 2019</li>
</ul>
<p>文章提出用双层的transformer来进行extractive summarization。双层也就是句子层和单词层。首先对模型先进行预训练。传统的预训练如ELMO，BERT都是预测单词进行训练，但是文章提出的是预测对应的句子。也就是说，对于一个文本n句话，随机选择15%的句子进行mask处理【处理中90%情况，直接用MASK token代替被选中的句子中的每个word，另10%情况是用另一句话来代替被选中的那句话】。然后我们的模型对mask的句子进行预测句子中的每个单词，来使得NLL最小化。生成单词的方式是结合句子中目前已经生成的单词和句子层的句子vector用transformer来对单词进行一个概率分布。</p>
<p><img src="/2019/05/21/abstractive-summarization/7.png" width="60%"></p>
<p>对于正式模型进行extractive summarization时，是对每个句子判断他对应的label，是被选取还是不被选取。</p>
<p>真实预训练时候，模型先对一个GIGA-CM(Gigaword+CnnDm组合的)数据集进行大量的预训练，再用CNNDM进行小部分的预训练，然后用CNNDM进行summarizaitin的fine tuning。</p>
<p><img src="/2019/05/21/abstractive-summarization/8.png" width="60%"></p>
<p><br></p>
<h3 id="Bottom-Up-Abstractive-Summarization"><a href="#Bottom-Up-Abstractive-Summarization" class="headerlink" title="Bottom-Up Abstractive Summarization"></a>Bottom-Up Abstractive Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1808.10792.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.10792.pdf</a></li>
<li>Harvard University</li>
</ul>
<ul>
<li>EMNLP 2018</li>
</ul>
<p>文章最主要的思想就是一个：在使用pointer network的情况下，出现的摘要有很大的概率直接copy了原文中很长的一句话到summary中，但是在真实的summary中，很少出现长句子的copy。因此我对source article选择了部分词语被mask，被mask的这些单词就不能被选中copy到生成的summary中，这样就阻止了很长的句子直接被copy。</p>
<p>个人感觉：这个选择mask的单词没有一个标准答案啊，这很难训练吧？</p>
<p><br></p>
<h3 id="A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss"><a href="#A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss" class="headerlink" title="A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss"></a>A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1805.06266.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.06266.pdf</a></li>
<li>National Tsing Hua University</li>
<li>ACL 2018</li>
</ul>
<p>文章使用的是extractive和abstractive结合的摘要生成方法。他主要的思想是extractive部分获得每个句子不同的重要程度，作为句子层的attention weight，然后用句子层的attention weight去影响单词层的attention weight，获得updated 的attention weight。如下图所示。这样的思想主要是如果句子attention比较小的句子中的词语的attention值应该也比较小，就是比较不应该被生成。</p>
<p><img src="/2019/05/21/abstractive-summarization/9.png" width="40%"></p>
<p>然后对于loss的计算，分为四个部分，一个部分是选了pointer generator中的converage loss，一个是abstractive 部分的最后生成的每个单词的概率分布的loss计算，一个是extractor部分计算的attention weight的loss。就是作者会根据原文和参考摘要用rouge L计算每个句子的重要程度，来决定这个句子是不是应该被选中，选中则$g_n=1$,作为一个groud truth去计算extractor部分的loss。</p>
<p><img src="/2019/05/21/abstractive-summarization/10.png" width="50%"></p>
<p>最后一个loss部分是去计算word 的attention和sentence 的attention是不是一致的，也就是说希望word attention大的单词对应的句子的sentence attention也是比较大的。这边是每个decoder步选topk个attention高的word来计算。</p>
<p><img src="/2019/05/21/abstractive-summarization/11.png" width="50%"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/27/台湾/" rel="next" title="台湾旅游攻略">
                <i class="fa fa-chevron-left"></i> 台湾旅游攻略
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
     <div id="gitalk-container"></div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yunyan.hong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Robust-Abstractive-System-for-Cross-Lingual-Summarization"><span class="nav-number">1.</span> <span class="nav-text">A Robust Abstractive System for Cross-Lingual Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstractive-Summarization-of-Reddit-Posts-with-Multi-level-Memory-Networks"><span class="nav-number">2.</span> <span class="nav-text">Abstractive Summarization of Reddit Posts with Multi-level Memory Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SEQ3-Differentiable-Sequence-to-Sequence-to-Sequence-Autoencoder-for-Unsupervised-Abstractive-Sentence-Compression"><span class="nav-number">3.</span> <span class="nav-text">SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BI-DIRECTIONAL-ATTENTION-FLOW-FOR-MACHINE-COMPREHENSION"><span class="nav-number">4.</span> <span class="nav-text">BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Achieving-Open-Vocabulary-Neural-Machine-Translation-with-Hybrid-Word-Character-Models"><span class="nav-number">5.</span> <span class="nav-text">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fully-Character-Level-Neural-Machine-Translation-without-Explicit-Segmentation"><span class="nav-number">6.</span> <span class="nav-text">Fully Character-Level Neural Machine Translation without Explicit Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HL-EncDec-A-Hybrid-Level-Encoder-Decoder-for-Neural-Response-Generation"><span class="nav-number">7.</span> <span class="nav-text">HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization"><span class="nav-number">8.</span> <span class="nav-text">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bottom-Up-Abstractive-Summarization"><span class="nav-number">9.</span> <span class="nav-text">Bottom-Up Abstractive Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss"><span class="nav-number">10.</span> <span class="nav-text">A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yunyan.hong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '214b2aee446518ccee23',
          clientSecret: '6b5e9007abf73fbcdce7d46cc4ae66a5fa5e6a3d',
          repo: 'hongyunyan.github.io',
          owner: 'hongyunyan',
          admin: ['hongyunyan'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
