<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="A Robust Abstractive System for Cross-Lingual Summarization 文章地址：http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf Columbia University NAACL 2019  文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种">
<meta property="og:type" content="article">
<meta property="og:title" content="新一轮Summarization以及相关扫文工作">
<meta property="og:url" content="http://yoursite.com/2019/05/21/abstractive-summarization/index.html">
<meta property="og:site_name" content="Yunyan Hong">
<meta property="og:description" content="A Robust Abstractive System for Cross-Lingual Summarization 文章地址：http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf Columbia University NAACL 2019  文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/1.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/2.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/3.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/4.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/5.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/6.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/7.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/8.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/9.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/10.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/11.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/12.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/13.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/14.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/15.png">
<meta property="og:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/16.png">
<meta property="og:updated_time" content="2019-06-24T08:23:39.363Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="新一轮Summarization以及相关扫文工作">
<meta name="twitter:description" content="A Robust Abstractive System for Cross-Lingual Summarization 文章地址：http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf Columbia University NAACL 2019  文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种">
<meta name="twitter:image" content="http://yoursite.com/2019/05/21/abstractive-summarization/1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2019/05/21/abstractive-summarization/">





  <title>新一轮Summarization以及相关扫文工作 | Yunyan Hong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yunyan Hong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/21/abstractive-summarization/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yunyan.hong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yunyan Hong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">新一轮Summarization以及相关扫文工作</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-21T10:48:00+08:00">
                2019-05-21
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/Research/" itemprop="url" rel="index">
                    <span itemprop="name">Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="A-Robust-Abstractive-System-for-Cross-Lingual-Summarization"><a href="#A-Robust-Abstractive-System-for-Cross-Lingual-Summarization" class="headerlink" title="A Robust Abstractive System for Cross-Lingual Summarization"></a>A Robust Abstractive System for Cross-Lingual Summarization</h3><ul>
<li>文章地址：<a href="http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf" target="_blank" rel="noopener">http://www.cs.columbia.edu/~ouyangj/OuyangSongMcKeown2019.pdf</a></li>
<li>Columbia University</li>
<li>NAACL 2019</li>
</ul>
<p>文章主要的任务是指面对一个小语种的文本时，我如何生成一个大语种的摘要。在面对跨语种摘要的任务中，一般的选择有两种，一种是先翻译再摘要，另一种是先摘要再翻译。以前的研究中，因为先摘要再翻译可以减少计算量，所以选择的是先摘要后翻译。但是因为以前主要研究的是都是大语种，但是小语种容易出现没有足够的小语种训练集来训练一个优秀的摘要模型，因此作者在文中提出先翻译再摘要的做法。</p>
<p>作者的实验方式，是选用了The New York Times的数据集，将他们先翻译成三种小语种语言，再将翻译后的文本再翻译为英文。然后用这个翻译回来的noisy的英文文本去训练abstractor模型，同时还利用三种语言翻译回来的文本混合，训练一个mixed的模型。然后进行测试。测试结果这样的摘要结果优于直接用Pointer Generator的模型去对noisy文本进行摘要。【讲道理这不是很显然吗，一个都不符合你的training data的类型？】同时他还选用了真实的小语种语料来测试模型，并用人工判读的方式来测评摘要的内容和流畅度。另外作者还找了一个另一个小语言DUC数据，去测试不同的模型，来证明自己训练的模型的可行性。</p>
<p>文章主要做的事情可以理解为用抽象模型删除难翻译的短语并生成新文本的能力，该模型学会从不流畅的输入中产生流畅的摘要。</p>
<p>个人感受：这应该算一个蛮新的课题，不过他的模型上并没有什么新的内容，就只是用的是不流畅的文本。 </p>
<p><br></p>
<h3 id="Abstractive-Summarization-of-Reddit-Posts-with-Multi-level-Memory-Networks"><a href="#Abstractive-Summarization-of-Reddit-Posts-with-Multi-level-Memory-Networks" class="headerlink" title="Abstractive Summarization of Reddit Posts with Multi-level Memory Networks"></a>Abstractive Summarization of Reddit Posts with Multi-level Memory Networks</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1811.00783.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1811.00783.pdf</a></li>
<li>Seoul National University</li>
<li>NAACL 2019</li>
</ul>
<p>文章主要做了两件事情，一件事情是提出了一个新的数据集，Reddit TITU dataset，是从一个论坛上得到的数据集，每个文本对应一个长摘要和一个短摘要，并且数据集存在一个优势：新闻类的数据的重要信息普遍在开头的句子中，因此数据集中包含了一定的结构信息。但是这个数据集的重点信息没有明显的结构分布，所以更为普适。</p>
<p>另一件事情，是提出了一个新的摘要模型，Multi-Level Memory Network，感觉没有非常看懂这个神奇的模型，感觉最底层是一个多层的CNN，CNN层通过多个不同跳跃格数的卷积，生成对应的信息，写入memory cells。memory cells有两层，一层叫input，一层叫output。然后再用一个decoder来生成单词。【主要是memory networks没有看懂，以后跟着代码看吧】</p>
<p><img src="/2019/05/21/abstractive-summarization/1.png" width="100%" height="80%"></p>
<p><br></p>
<h3 id="SEQ3-Differentiable-Sequence-to-Sequence-to-Sequence-Autoencoder-for-Unsupervised-Abstractive-Sentence-Compression"><a href="#SEQ3-Differentiable-Sequence-to-Sequence-to-Sequence-Autoencoder-for-Unsupervised-Abstractive-Sentence-Compression" class="headerlink" title="SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression"></a>SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1904.03651.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1904.03651.pdf</a></li>
<li>National Technical University of Athens</li>
<li>NAACL 2019</li>
</ul>
<p>文章做的是一个无监督的文本摘要（也就是文本压缩），利用的是一个类似Autoencoder的结构，前面一部分是对文本的压缩，后一部分是根据摘要结果对文本进行重建。然后计算的方式都是Seq2Seq的结构。计算的loss一个是输入文本和重建文本的diff，一个是topic的diff（原文本和压缩后的文本，用TF-IDF计算),摘要和language model的KL分布diff，长度diff四个部分。</p>
<p><img src="/2019/05/21/abstractive-summarization/2.png" width="60%" height="80%"></p>
<p><br></p>
<h3 id="BI-DIRECTIONAL-ATTENTION-FLOW-FOR-MACHINE-COMPREHENSION"><a href="#BI-DIRECTIONAL-ATTENTION-FLOW-FOR-MACHINE-COMPREHENSION" class="headerlink" title="BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION"></a>BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1611.01603.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1611.01603.pdf</a></li>
</ul>
<ul>
<li>Allen Institute for Artificial Intelligence,Washington</li>
<li>ICLR，2017</li>
</ul>
<p>文章主要的任务是通过阅读理解来回答question。也就是说给定一个文本，再给一个问题，让系统学习回答正确的答案。</p>
<p>模型一共分为6层，如图所示：</p>
<p><img src="/2019/05/21/abstractive-summarization/3.png" width="100%" height="80%"></p>
<p>最下面三层主要的工作就是一个embedding的工作，一层是character embedding层，用字符级的CNN，CNN在整个宽度上max-pooling。另一层是一个word embedding层，用Glove预训练的词向量。然后这两个输入拼接起来输入Highway Network得到对应的context vector X(d,T)和query vector Q(d,J)。然后根据得到的vector通过bi-LSTM，获得对应的输出，H(2d, T), U(2d, J)。通过这样的编码操作，我们已经可以得到context和query整合了不同层次的表征信息。</p>
<p>下一层是Attention Flow Layer，输入是上层得到的H和U，输出是具有问题感知的上下文表征G。在这层中我们是要计算两个方向的attention : context to query 以及query to context。首先我们要先计算一个这两个attention都要用到的一个相似矩阵。$S_{tj} = \alpha (H_{:t}, U_{:j})$ 。其中$S_{tj}$表明的是第t个上下文中的单词和第j个query中的单词的相似度，其中$\alpha(h,u) = W_S^T[h;u;h \circ u]$。</p>
<p>Context-to-query Attention(C2Q)计算的是对每一个 context word 而言哪些 query words 和它最相关，所以$a_t = softmax(S_{t:})$, $\tilde{U_{:t}} = \sum_j a_{tj}U_{:j}$, 所以$\tilde{U}$是一个2d*T的矩阵，一个全新的问题编码。</p>
<p>Query-to-context Attention(Q2C)计算的是对每一个 query word 而言哪些 context words 和它最相关,因此这些单词就和答案有很大的关系。所以直接取了S矩阵中最大的那列（也就是那个单词对我们query的相似性最大）$b=softmax(max_{col}(S))$, $\tilde{h} = \sum_t b_t H_{:t}$,然后重复T次得到$\tilde{H}$,也是2d*T。</p>
<p>随后我们将这两个新编码经过一个MLP函数\beta的计算得到G， $G_{:t} = \beta(H_{:t}, \tilde{U_{:t}}, \tilde{H_{:t}})$, $\beta(h, \tilde{u}, \tilde{h}) = [h;\tilde{u}; h \circ \tilde{u}; h \circ \tilde{h}]$。（8d*T） </p>
<p>G通过一个BiLSTM得到M（2d*T），来捕获矩阵在时序上的依赖关系。最后的output层就是输出起始位置和结束为止的概率分布。$p^1 = softmax(W^T_{p1}[G;M])$, $p^2 = softmax(W^T_{p2}[G;M])$，损失函数是$L(\Theta ) = -1/N \sum_i^N[log(p1_{y1}) + log(p2_{y2})]$。</p>
<p>整体而言，文章的亮点是那个双向的attention思想，btw也是我第一次见到word embedding和char embedding组合的方式。</p>
<p><br></p>
<h3 id="Achieving-Open-Vocabulary-Neural-Machine-Translation-with-Hybrid-Word-Character-Models"><a href="#Achieving-Open-Vocabulary-Neural-Machine-Translation-with-Hybrid-Word-Character-Models" class="headerlink" title="Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models"></a>Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1604.00788.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1604.00788.pdf</a></li>
<li>Stanford， Christopher D. Manning</li>
<li>ACL 2016</li>
</ul>
<p>文章的主要观点就是提出了下面这个模型，也就是说在输入或输出word层为unk的地方，加入一个char层进行embedding和decoder。这样来解决char层又慢又容易出现错误单词，word层有unk出现的问题。</p>
<p><img src="/2019/05/21/abstractive-summarization/4.png" width="50%" height="80%"></p>
<p>对于在char层的输入的states，作者提出，因为原先word层用于softmax的h‘，因为都是生成了unk，有很大概率比较相近，很容易产生相同的char，所以作者提出利用context c和hidden states单独训练用于char层输入的h’‘。整体格式和训练h’相同，只是是不同的W而已。</p>
<p><br></p>
<h3 id="Fully-Character-Level-Neural-Machine-Translation-without-Explicit-Segmentation"><a href="#Fully-Character-Level-Neural-Machine-Translation-without-Explicit-Segmentation" class="headerlink" title="Fully Character-Level Neural Machine Translation without Explicit Segmentation"></a>Fully Character-Level Neural Machine Translation without Explicit Segmentation</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1610.03017.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1610.03017.pdf</a></li>
<li>ETH Zu ̈rich</li>
<li>2017 【 这篇看的比较简略，和A Character-Level Decoder without Explicit Segmentation<br>for Neural Machine Translation是一波人写的，想法基本一致的】</li>
</ul>
<p>文章利用完全的char层进行NMT工作，这个图就是对输入信息的encoder方式，用CNN+highway network等进行encoder。decoder用的是<a href="https://arxiv.org/pdf/1603.06147.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1603.06147.pdf</a>这篇文章中的。</p>
<p><img src="/2019/05/21/abstractive-summarization/5.png" width="100%" height="80%"></p>
<p><br></p>
<h3 id="HL-EncDec-A-Hybrid-Level-Encoder-Decoder-for-Neural-Response-Generation"><a href="#HL-EncDec-A-Hybrid-Level-Encoder-Decoder-for-Neural-Response-Generation" class="headerlink" title="HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation"></a>HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation</h3><ul>
<li>文章地址：<a href="https://www.aclweb.org/anthology/C18-1072" target="_blank" rel="noopener">https://www.aclweb.org/anthology/C18-1072</a></li>
<li>北京大学</li>
<li>ACL 2018</li>
</ul>
<p>用word层和char层一起生成语句回复。做的是中文的数据。用下图结构生成word 的embedding。用charCNN生成的embedding和word 本身的embedding相加，作为encoder结果。decoder就是要么生成word，要么就生成char，来避免oov。</p>
<p><img src="/2019/05/21/abstractive-summarization/6.png" width="80%" height="80%"></p>
<p><br></p>
<h3 id="HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization"><a href="#HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization" class="headerlink" title="HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization"></a>HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1905.06566.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.06566.pdf</a></li>
<li>MSRA，Beijing</li>
<li>ACL 2019</li>
</ul>
<p>文章提出用双层的transformer来进行extractive summarization。双层也就是句子层和单词层。首先对模型先进行预训练。传统的预训练如ELMO，BERT都是预测单词进行训练，但是文章提出的是预测对应的句子。也就是说，对于一个文本n句话，随机选择15%的句子进行mask处理【处理中90%情况，直接用MASK token代替被选中的句子中的每个word，另10%情况是用另一句话来代替被选中的那句话】。然后我们的模型对mask的句子进行预测句子中的每个单词，来使得NLL最小化。生成单词的方式是结合句子中目前已经生成的单词和句子层的句子vector用transformer来对单词进行一个概率分布。</p>
<p><img src="/2019/05/21/abstractive-summarization/7.png" width="60%"></p>
<p>对于正式模型进行extractive summarization时，是对每个句子判断他对应的label，是被选取还是不被选取。</p>
<p>真实预训练时候，模型先对一个GIGA-CM(Gigaword+CnnDm组合的)数据集进行大量的预训练，再用CNNDM进行小部分的预训练，然后用CNNDM进行summarizaitin的fine tuning。</p>
<p><img src="/2019/05/21/abstractive-summarization/8.png" width="60%"></p>
<p><br></p>
<h3 id="Bottom-Up-Abstractive-Summarization"><a href="#Bottom-Up-Abstractive-Summarization" class="headerlink" title="Bottom-Up Abstractive Summarization"></a>Bottom-Up Abstractive Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1808.10792.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1808.10792.pdf</a></li>
<li>Harvard University</li>
</ul>
<ul>
<li>EMNLP 2018</li>
</ul>
<p>文章最主要的思想就是一个：在使用pointer network的情况下，出现的摘要有很大的概率直接copy了原文中很长的一句话到summary中，但是在真实的summary中，很少出现长句子的copy。因此我对source article选择了部分词语被mask，被mask的这些单词就不能被选中copy到生成的summary中，这样就阻止了很长的句子直接被copy。</p>
<p>个人感觉：这个选择mask的单词没有一个标准答案啊，这很难训练吧？</p>
<p><br></p>
<h3 id="A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss"><a href="#A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss" class="headerlink" title="A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss"></a>A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1805.06266.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.06266.pdf</a></li>
<li>National Tsing Hua University</li>
<li>ACL 2018</li>
</ul>
<p>文章使用的是extractive和abstractive结合的摘要生成方法。他主要的思想是extractive部分获得每个句子不同的重要程度，作为句子层的attention weight，然后用句子层的attention weight去影响单词层的attention weight，获得updated 的attention weight。如下图所示。这样的思想主要是如果句子attention比较小的句子中的词语的attention值应该也比较小，就是比较不应该被生成。</p>
<p><img src="/2019/05/21/abstractive-summarization/9.png" width="40%"></p>
<p>然后对于loss的计算，分为四个部分，一个部分是选了pointer generator中的converage loss，一个是abstractive 部分的最后生成的每个单词的概率分布的loss计算，一个是extractor部分计算的attention weight的loss。就是作者会根据原文和参考摘要用rouge L计算每个句子的重要程度，来决定这个句子是不是应该被选中，选中则$g_n=1$,作为一个groud truth去计算extractor部分的loss。</p>
<p><img src="/2019/05/21/abstractive-summarization/10.png" width="50%"></p>
<p>最后一个loss部分是去计算word 的attention和sentence 的attention是不是一致的，也就是说希望word attention大的单词对应的句子的sentence attention也是比较大的。这边是每个decoder步选topk个attention高的word来计算。</p>
<p><img src="/2019/05/21/abstractive-summarization/11.png" width="50%"></p>
<p><br></p>
<h3 id="A-Discourse-Aware-Attention-Model-for-Abstractive-Summarization-of-Long-Documents"><a href="#A-Discourse-Aware-Attention-Model-for-Abstractive-Summarization-of-Long-Documents" class="headerlink" title="A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents"></a>A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1804.05685.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.05685.pdf</a></li>
<li>Adobe Research</li>
<li>NAACL 2018</li>
</ul>
<p>文章的摘要对象是科研论文，如在arxiv上的论文。感觉模型整体 不是很有创意，就是一个章节层，一个词语层，算attention的时候，不仅要算word的attention，还要算跟section的attention，并且运用了copynet的想法去解决oov问题。</p>
<p><br></p>
<h3 id="MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation"><a href="#MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation" class="headerlink" title="MASS: Masked Sequence to Sequence Pre-training for Language Generation"></a>MASS: Masked Sequence to Sequence Pre-training for Language Generation</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1905.02450.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.02450.pdf</a></li>
</ul>
<ul>
<li>MSRA</li>
<li>ICML 2019</li>
</ul>
<p>文章提出是一个用于seq2seq模型的masked预训练模型，用于进行language generation。如BERT这些模型，其实等于只是预训练了一个encoder或者一个decoder，如果要用在seq2seq上，则要两个单独训练，效果并不好。因此这边提出了用一个encoder decoder的结构的进行预训练，然后用于language generation。</p>
<p><img src="/2019/05/21/abstractive-summarization/12.png" width="80%"></p>
<p>在输入中mask掉长度为k的单词，然后再decoder中生成这些被mask的词语，并且在decoder中没有mask的词语并不会有对应的input信息进行提示，这样来帮助整个encoder，decoder模型更好的被pretrain。然后长度为k的k在训练中是取不同值来进行不同性质的训练的。</p>
<p>问题：如何fine tune？我感觉是直接fine tune啊？？</p>
<p><br></p>
<h3 id="GRAPH2SEQ-GRAPH-TO-SEQUENCE-LEARNING-WITH-ATTENTION-BASED-NEURAL-NETWORKS"><a href="#GRAPH2SEQ-GRAPH-TO-SEQUENCE-LEARNING-WITH-ATTENTION-BASED-NEURAL-NETWORKS" class="headerlink" title="GRAPH2SEQ: GRAPH TO SEQUENCE LEARNING WITH ATTENTION-BASED NEURAL NETWORKS"></a>GRAPH2SEQ: GRAPH TO SEQUENCE LEARNING WITH ATTENTION-BASED NEURAL NETWORKS</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1804.00823.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.00823.pdf</a></li>
<li>IBM</li>
<li>arxiv，2018</li>
</ul>
<p>文章提出用图-序列的Graph2Seq模型来处理对应的具有明显结构性的任务。下图为整个模型的结构图，首先我们先要计算每个node的embedding，计算方式为：</p>
<ul>
<li>先初始化每个节点，根据节点原本的信息，比如map到matrix计算啊，或者本身经过LSTM等。</li>
<li>然后在有向图中一个节点分为前向节点和后向节点，每一轮，分别更新所有节点，更新方法是把这个节点相关的节点信息结合起来（结合的方法有直接取平均，或随机排列后输入LSTM，或各自经过前向神经网络，然后maxpooling。</li>
<li>迭代k轮以后，把前向和后向节点concat得到node embedding。</li>
</ul>
<p>然后我们根据node embedding获得graph embedding，graph embedding也有两个方法，一个是经过fully connecetd layer然后max pooling获得对应的表示；另一个就是加一个附加节点，然后链接所有节点，获取对应的表示。</p>
<p>获得对应的graph embedding后，我们可以来decoder了。在decoder的时候我们也会进行attention计算，每个隐藏层状态和node embedding求attention weight获取对应的context vector，作为attention相关表示，帮助decoder。</p>
<p>对应的实验是进行了三个：</p>
<ul>
<li>bAbI任务：一个简单推理任务</li>
<li>最短路径任务：输入为一个有向图，一个开始节点与一个结束节点，输出为连接开始节点与结束节点的路径（使用节点序列进行表示）。</li>
<li>自然语言生成任务：输入为SQL语句，输出为相应的文本。</li>
</ul>
<p><img src="/2019/05/21/abstractive-summarization/13.png" width="90%"></p>
<p><br></p>
<h3 id="Tree-to-Sequence-Attentional-Neural-Machine-Translation"><a href="#Tree-to-Sequence-Attentional-Neural-Machine-Translation" class="headerlink" title="Tree-to-Sequence Attentional Neural Machine Translation"></a>Tree-to-Sequence Attentional Neural Machine Translation</h3><ul>
<li>文章地址：<a href="https://www.aclweb.org/anthology/P16-1078" target="_blank" rel="noopener">https://www.aclweb.org/anthology/P16-1078</a></li>
<li>The University of Tokyo</li>
<li>ACL 2016</li>
</ul>
<p>文章提出encoder部分不仅有 Sequential Encoder还要有Tree-based Encoder，用于学习输入文本的结构信息。Tree-based Encoder可以理解为一个二叉树结构，用tree-LSTM进行一层层的计算。在decoder的时候也是利用两个 encoder的信息，也用两个encoder来做attention处理。</p>
<p><img src="/2019/05/21/abstractive-summarization/14.png" width="60%"></p>
<p>然后他decoder中最后softmax步骤用的是blackout的方式【BLACKOUT: SPEEDING UP RECURRENT NEURAL NET- WORK LANGUAGE MODELS WITH VERY LARGE VO- CABULARIES，ICLR2016】大概思想就是取k个词语来代替分母中对所有词语的概率计算加和。—论文还论述了这个方法和优化softmax的另两个方法NOISE CONTRASTIVE ESTIMATION和IMPORTANCE SAMPLING的区别，但是这两个神仙方法我不是很看懂了，只感觉第一个的区别在与和本方法中k取值概率分布的差距，第二个感觉思路就是不同的啊，使用一部分的概率分布来作为全部的分布？？？？待讨论哎</p>
<ul>
<li>On Using Very Large Target Vocabulary for Neural Machine Translation（2015）这篇文章也是做sample softmax的问题的，他的想法就是把V分成几个子集，然后归一化的计算只计算小V的2333.</li>
</ul>
<p><br></p>
<h3 id="Actor-Critic-based-Training-Framework-for-Abstractive-Summarization"><a href="#Actor-Critic-based-Training-Framework-for-Abstractive-Summarization" class="headerlink" title="Actor-Critic based Training Framework for Abstractive Summarization"></a>Actor-Critic based Training Framework for Abstractive Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1803.11070.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.11070.pdf</a></li>
<li>The Chinese University of Hong Kong</li>
<li>2018，arxiv</li>
</ul>
<p>文章最主要的想法，就是loss函数不能只包含NLL，还要有其他的来判断摘要的质量，所以他用了一个网络做二分类来判别生成摘要的质量，【不是GAN那种形式，一起调节那样】可以理解为先单独训练二分类，然后用它来做loss部分。</p>
<p>下图为整个模型的训练方式，也就是先用NLL来预训练一个模型，然后训练一下二分类网络，然后用NLL和网络共同fine tune 模型，然后整个流程重复。</p>
<p><img src="/2019/05/21/abstractive-summarization/15.png" width="50%"></p>
<p><br></p>
<h3 id="Multi-Reward-Reinforced-Summarization-with-Saliency-and-Entailment"><a href="#Multi-Reward-Reinforced-Summarization-with-Saliency-and-Entailment" class="headerlink" title="Multi-Reward Reinforced Summarization with Saliency and Entailment"></a>Multi-Reward Reinforced Summarization with Saliency and Entailment</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1804.06451.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.06451.pdf</a></li>
<li>UNC Chapel Hill</li>
<li>NAACL 2018</li>
</ul>
<p>文章主要的思想就是提出了两个评判标准，用这两个评判的reward来训练这个模型，一个reward为Saliency Reward，也就是在算ROUGE的时候不对每个单词同等对待，根据每个单词的重要程度不同，给予不同的权重。这个重要的程度是用一个encoder+softmax去求的的。另一个reward是Entailment Reward，考虑的是生成的摘要包含的信息相关度吧，计算groud truth和生成摘要的每句话的关系求平均，然后再乘以单词个数比例来作为reward值—这个part可以理解为</p>
<blockquote>
<p>文本蕴含 (Text Entailment) 任务的简化，它的任务形式为：给定一个前提 (Premise) ，根据这个前提去推断假设 (Hypothesis) 与前提的关系。该任务的关系分为三种，蕴含关系 (Entailment)、矛盾关系 (Contradiction) 以及中立关系 (Neutral)</p>
</blockquote>
<p><br></p>
<h3 id="BiSET-Bi-directional-Selective-Encoding-with-Template-for-Abstractive-Summarization"><a href="#BiSET-Bi-directional-Selective-Encoding-with-Template-for-Abstractive-Summarization" class="headerlink" title="BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization"></a>BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1906.05012.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.05012.pdf</a></li>
<li>中山大学</li>
<li>ACL 2019</li>
</ul>
<p>这篇文章主要结合了两篇文章的思想，分别是BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION 和 Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization。主要的想法就是用template来辅助生成摘要，然后这边不同的是得到template以后借用BI-Direction的attention来组合template和原文的内容，来生成摘要结果。【感觉模型很复杂2333】</p>
<p><br></p>
<h3 id="Scoring-Sentence-Singletons-and-Pairs-for-Abstractive-Summarization"><a href="#Scoring-Sentence-Singletons-and-Pairs-for-Abstractive-Summarization" class="headerlink" title="Scoring Sentence Singletons and Pairs for Abstractive Summarization"></a>Scoring Sentence Singletons and Pairs for Abstractive Summarization</h3><ul>
<li>文章地址：<a href="https://arxiv.org/pdf/1906.00077.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1906.00077.pdf</a></li>
</ul>
<ul>
<li>University of Central Florida</li>
<li>ACL 2019</li>
</ul>
<p>这篇文章的主要做法就是在文中选出重要的单个句子或两个句子组合，然后对高分选项进行生成对应的summary，进行组合获得文章整体的摘要结果。</p>
<p>文章中用来表示每个句子或者每一对句子的方法有两种，一种是用BERT的方法，另一种是用传统的VSM</p>
<p>（一个Vector space的模型）得到所有的可能备选项以后，用线性组合求的他们的对应的分数，每次都选择分数高，且和目前已选内容重复较小的备选项。重复较小的计算方式是计算备选项和目前摘要的cos值。然后再用pointer Generator来生成摘要。对于Pointer Generator的训练，我们并不是利用dataset中的pair来训练，而是用ground-truth instances来进行训练，感觉就是选用truth对应的原文句子或句子组来训练pointer generator？（这个part我也不是很懂，感觉文章写的意思是根据human summary？）</p>
<p>整个摘要生成的过程如下。文章实验做出来效果提高了很多，主要是在selection部分的结果非常好</p>
<p><img src="/2019/05/21/abstractive-summarization/16.png" width="70%"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/27/台湾/" rel="next" title="台湾旅游攻略">
                <i class="fa fa-chevron-left"></i> 台湾旅游攻略
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/08/06/arch/" rel="prev" title="系统方向相关学习">
                系统方向相关学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
     <div id="gitalk-container"></div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yunyan.hong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">16</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Robust-Abstractive-System-for-Cross-Lingual-Summarization"><span class="nav-number">1.</span> <span class="nav-text">A Robust Abstractive System for Cross-Lingual Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Abstractive-Summarization-of-Reddit-Posts-with-Multi-level-Memory-Networks"><span class="nav-number">2.</span> <span class="nav-text">Abstractive Summarization of Reddit Posts with Multi-level Memory Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SEQ3-Differentiable-Sequence-to-Sequence-to-Sequence-Autoencoder-for-Unsupervised-Abstractive-Sentence-Compression"><span class="nav-number">3.</span> <span class="nav-text">SEQ3: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BI-DIRECTIONAL-ATTENTION-FLOW-FOR-MACHINE-COMPREHENSION"><span class="nav-number">4.</span> <span class="nav-text">BI-DIRECTIONAL ATTENTION FLOW FOR MACHINE COMPREHENSION</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Achieving-Open-Vocabulary-Neural-Machine-Translation-with-Hybrid-Word-Character-Models"><span class="nav-number">5.</span> <span class="nav-text">Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fully-Character-Level-Neural-Machine-Translation-without-Explicit-Segmentation"><span class="nav-number">6.</span> <span class="nav-text">Fully Character-Level Neural Machine Translation without Explicit Segmentation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HL-EncDec-A-Hybrid-Level-Encoder-Decoder-for-Neural-Response-Generation"><span class="nav-number">7.</span> <span class="nav-text">HL-EncDec: A Hybrid-Level Encoder-Decoder for Neural Response Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HIBERT-Document-Level-Pre-training-of-Hierarchical-Bidirectional-Transformers-for-Document-Summarization"><span class="nav-number">8.</span> <span class="nav-text">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bottom-Up-Abstractive-Summarization"><span class="nav-number">9.</span> <span class="nav-text">Bottom-Up Abstractive Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Unified-Model-for-Extractive-and-Abstractive-Summarization-using-Inconsistency-Loss"><span class="nav-number">10.</span> <span class="nav-text">A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Discourse-Aware-Attention-Model-for-Abstractive-Summarization-of-Long-Documents"><span class="nav-number">11.</span> <span class="nav-text">A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MASS-Masked-Sequence-to-Sequence-Pre-training-for-Language-Generation"><span class="nav-number">12.</span> <span class="nav-text">MASS: Masked Sequence to Sequence Pre-training for Language Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRAPH2SEQ-GRAPH-TO-SEQUENCE-LEARNING-WITH-ATTENTION-BASED-NEURAL-NETWORKS"><span class="nav-number">13.</span> <span class="nav-text">GRAPH2SEQ: GRAPH TO SEQUENCE LEARNING WITH ATTENTION-BASED NEURAL NETWORKS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tree-to-Sequence-Attentional-Neural-Machine-Translation"><span class="nav-number">14.</span> <span class="nav-text">Tree-to-Sequence Attentional Neural Machine Translation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Actor-Critic-based-Training-Framework-for-Abstractive-Summarization"><span class="nav-number">15.</span> <span class="nav-text">Actor-Critic based Training Framework for Abstractive Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-Reward-Reinforced-Summarization-with-Saliency-and-Entailment"><span class="nav-number">16.</span> <span class="nav-text">Multi-Reward Reinforced Summarization with Saliency and Entailment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BiSET-Bi-directional-Selective-Encoding-with-Template-for-Abstractive-Summarization"><span class="nav-number">17.</span> <span class="nav-text">BiSET: Bi-directional Selective Encoding with Template for Abstractive Summarization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Scoring-Sentence-Singletons-and-Pairs-for-Abstractive-Summarization"><span class="nav-number">18.</span> <span class="nav-text">Scoring Sentence Singletons and Pairs for Abstractive Summarization</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yunyan.hong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">

  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '214b2aee446518ccee23',
          clientSecret: '6b5e9007abf73fbcdce7d46cc4ae66a5fa5e6a3d',
          repo: 'hongyunyan.github.io',
          owner: 'hongyunyan',
          admin: ['hongyunyan'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>



  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
